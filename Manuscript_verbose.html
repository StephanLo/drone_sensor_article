<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Albertus S. Louw">
<meta name="author" content="Chen Xinyu">
<meta name="author" content="Ram Avtar">

<title>On the accuracy of an infrared-converted drone camera with Orange-Cyan-NIR filter for use in vegetation and environmental monitoring</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="Manuscript_verbose_files/libs/clipboard/clipboard.min.js"></script>
<script src="Manuscript_verbose_files/libs/quarto-html/quarto.js"></script>
<script src="Manuscript_verbose_files/libs/quarto-html/popper.min.js"></script>
<script src="Manuscript_verbose_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Manuscript_verbose_files/libs/quarto-html/anchor.min.js"></script>
<link href="Manuscript_verbose_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Manuscript_verbose_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Manuscript_verbose_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Manuscript_verbose_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Manuscript_verbose_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

<script src="Manuscript_verbose_files/libs/kePrint-0.0.1/kePrint.js"></script>
<link href="Manuscript_verbose_files/libs/lightable-0.0.1/lightable.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="Manuscript_verbose.docx"><i class="bi bi-file-word"></i>MS Word</a></li><li><a href="Manuscript_verbose.pdf"><i class="bi bi-file-pdf"></i>PDF (elsevier)</a></li></ul></div></div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">On the accuracy of an infrared-converted drone camera with Orange-Cyan-NIR filter for use in vegetation and environmental monitoring</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Albertus S. Louw </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Graduate School of Environmental Science, Hokkaido University
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Chen Xinyu </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Graduate School of Environmental Science, Hokkaido University
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Ram Avtar </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Graduate School of Environmental Science, Hokkaido University
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  

</header>

<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>Drones equipped with cameras sensitive to near-infrared wavelengths are increasingly being used in environmental assessment studies and in agriculture. These cameras are sensitive to vegetation cover, extent of eutrophication in water bodies, and aspects of crops such as growth vigour, biomass and potential yield. Single-sensor (‘RGB’) cameras with modified spectral filters that allow for capturing near-infrared wavelengths offer a low-cost alternative to multi-sensor multispectral cameras or spectrometers. However, some studies point to lower measurement accuracies by such infrared converted sensors. So, to what extent can infrared converted cameras be used to quantify vegetation condition? This case study compared Normalized Difference Vegetation Index (NDVI) measurements from an infrared converted camera to those measured by a drone-borne multispectral camera and a handheld NDVI meter, as captured over soybean and potato fields. It was observed that the infrared converted camera derived NDVI was consistently lower over vegetation than NDVI measured from the multispectral and handheld sensors. The study builds on previous case studies with similar results by further evaluating the reflectance patterns of the individual image bands to find possible reasons for the discrepancy in vegetation index measurements. There is good agreement between the near-infrared bands of the respective sensors (<span class="math inline">\(r = 0.87\)</span>), but the respective red bands have weak correlation (<span class="math inline">\(r = -0.03\)</span>). We discuss possible reasons for the lower vegetation index measurements observed by the infrared converted camera, noting broad band sensitivities and differing central wavelengths, which may have caused overestimated reflectance in the red band. All processing and analysis were executed with open-source software, and source code is made available to support reproducible research.</p>
<p>keywords: modified RGB camera, drone remote sensing, sensor comparison, UAV environmental monitoring, Mapir Survey 3W, MicaSense RedEdge-M.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Drones equipped with cameras are increasingly being used in environmental assessment studies and agriculture. For example, such drone-camera systems have recently been used to monitor ocean algal blooms <span class="citation" data-cites="fernandez-figueroa2022">(<a href="#ref-fernandez-figueroa2022" role="doc-biblioref">Fernandez-Figueroa, Wilson, and Rogers 2022</a>)</span> and other studies of water eutrophication <span class="citation" data-cites="barajas2021 sheng2021">(<a href="#ref-barajas2021" role="doc-biblioref">Barajas et al. 2021</a>; <a href="#ref-sheng2021" role="doc-biblioref">Sheng, Azhari, and Ibrahim 2021</a>)</span>. Drone cameras are also used to measure different aspects of crops, such as growth vigor, biomass and water-stress <span class="citation" data-cites="hafeez2022">(<a href="#ref-hafeez2022" role="doc-biblioref">Hafeez et al. 2022</a>)</span>. Specialized cameras for vegetation monitoring often have a sensor sensitive to wavelengths in the near-infrared part of the EM-spectrum. Healthy photosynthesizing vegetation shows high reflectance in near-infrared wavelengths, but comparatively low reflectance in the red part of the spectrum <span class="citation" data-cites="myneni1995">(<a href="#ref-myneni1995" role="doc-biblioref">Myneni et al. 1995</a>)</span>. So, if red and near-infrared wavelengths are recorder by a drone sensor, the condition or growth vigour of vegetation can be estimated with vegetation indices such as the Normalised Difference Vegetation Index, or NDVI <span class="citation" data-cites="huang2021">(<a href="#ref-huang2021" role="doc-biblioref">Huang et al. 2021</a>)</span>. This index ranges from -1 to 1, with higher values interpreted as vegetation with higher growth vigour. Such drone-camera systems offer a comparatively low-cost method to capture image data for wide areas, and allows data to be spatially referenced so that it can be overlaid with other sources of spatially explicit data. Different drone camera sensors however have different price-points and characteristics that may influence the quality of measurements <span class="citation" data-cites="nijland2014">(<a href="#ref-nijland2014" role="doc-biblioref">Nijland et al. 2014</a>)</span>. Thus, as the use of these technologies scale in agriculture and environmental studies, it is all the more necessary to evaluate the measurement bias or limitations of different drone sensor types.</p>
<p>There are different ways in which cameras are designed to capture near-infrared wavelengths <span class="citation" data-cites="maes2019">(<a href="#ref-maes2019" role="doc-biblioref">Maes and Steppe 2019</a>)</span>. One approach is that the camera has an independent imaging sensor and lens for each band. An alternative approach is where a single sensor camera that uses a Bayer Color filter array to capture 3 color bands – often referred to as red-green-blue (RGB) cameras – is modified to become sensitive to light in the near-infrared spectrum <span class="citation" data-cites="lebourgeois2008">(<a href="#ref-lebourgeois2008" role="doc-biblioref">Lebourgeois et al. 2008</a>)</span>. In this article these are referred to as infrared converted cameras <span class="citation" data-cites="nijland2014">(<a href="#ref-nijland2014" role="doc-biblioref">Nijland et al. 2014</a>)</span>, although they are also referred to as modified RGB <span class="citation" data-cites="lebourgeois2008 wang2020">(<a href="#ref-lebourgeois2008" role="doc-biblioref">Lebourgeois et al. 2008</a>; <a href="#ref-wang2020" role="doc-biblioref">Wang and Brinker 2020</a>)</span> or modified multispectral cameras <span class="citation" data-cites="fernandez-figueroa2022">(<a href="#ref-fernandez-figueroa2022" role="doc-biblioref">Fernandez-Figueroa, Wilson, and Rogers 2022</a>)</span>. Infrared converted cameras work by removing the filter which blocks NIR light from entering the sensor, and then substituting one of the RGB camera’s bands for the NIR band. For example, instead of Red-Green-Blue, the camera becomes sensitive to Red-Green-NIR. The single sensor infrared converted cameras are cheaper (by order of magnitude) than multispectral cameras with multiple sensors. They thus pose an attractive alternative, especially in cases where ‘proper’ multispectral cameras are considered prohibitively expensive. Several studies highlight the value of lowering the cost of technologies that can support environmental monitoring <span class="citation" data-cites="fernandez-figueroa2022">(<a href="#ref-fernandez-figueroa2022" role="doc-biblioref">Fernandez-Figueroa, Wilson, and Rogers 2022</a>)</span> and agriculture <span class="citation" data-cites="fernandez-gallego2019 cucho-padin2020 corti2019">(<a href="#ref-fernandez-gallego2019" role="doc-biblioref">Fernandez-Gallego et al. 2019</a>; <a href="#ref-cucho-padin2020" role="doc-biblioref">Cucho-Padin et al. 2020</a>; <a href="#ref-corti2019" role="doc-biblioref">Corti et al. 2019</a>)</span>.</p>
<p>Infrared converted cameras are an appealing option for drone agriculture remote sensing because of their comparative low-cost and ability to capture near-infrared wavelengths. However, it is necessary to verify the accuracy of spectral measurements made by these sensors. Despite being used in studies <span class="citation" data-cites="lebourgeois2008 argolodossantos2020">(<a href="#ref-lebourgeois2008" role="doc-biblioref">Lebourgeois et al. 2008</a>; <a href="#ref-argolodossantos2020" role="doc-biblioref">Argolo dos Santos et al. 2020</a>)</span>, some authors reported lower measurement accuracy for infrared converted cameras, if compared to other multispectral cameras or spectroscopes <span class="citation" data-cites="vonbueren2015 gomes2021 nijland2014">(<a href="#ref-vonbueren2015" role="doc-biblioref">Bueren et al. 2015</a>; <a href="#ref-gomes2021" role="doc-biblioref">Gomes et al. 2021</a>; <a href="#ref-nijland2014" role="doc-biblioref">Nijland et al. 2014</a>)</span>. This may partially be because the bands captured by single-sensor RGB camera is usually sensitive to light outside of the target wavelengths, and so measurements in specific band may be polluted by light in other parts of the spectrum <span class="citation" data-cites="burggraaff2019 berra2015">(<a href="#ref-burggraaff2019" role="doc-biblioref">Burggraaff et al. 2019</a>; <a href="#ref-berra2015" role="doc-biblioref">Berra et al. 2015</a>)</span>. This means for example that a modified RGB camera may report incorrect values for a specific band, because the sensor is also capturing light from the neighboring bands.</p>
<p>Before such infrared converted cameras can be recommended for quantitative environmental monitoring studies, or operational use on farms, it is important to verify that spectral measurements and vegetation indices derived from the infrared converted camera correspond well to measurements made by multispectral cameras, or hand-held spectrometers. The study by <span class="citation" data-cites="gomes2021">Gomes et al. (<a href="#ref-gomes2021" role="doc-biblioref">2021</a>)</span> investigated this dynamic for the Mapir Survey 3W commercial infrared converted camera, by comparing it to the multispectral MicaSense RedEdge-M camera. The study calculated the vegetation index NDVI of a coffee plantation using both cameras, as well as a handheld NDVI sensor. It was observed that NDVI measurements made by the infrared converted camera were consistently lower, if compared to the multispectral camera and handheld NDVI sensor. This finding may have important repercussions for the operational use of such infrared converted cameras. The results reported in the study by <span class="citation" data-cites="gomes2021">Gomes et al. (<a href="#ref-gomes2021" role="doc-biblioref">2021</a>)</span> may be sensitive to factors like light exposure at the time of measurement, time of the day, radiometric calibration errors, and camera settings. Thus, to strengthen the validity of these findings follow up studies can replicate their experiments, while varying components such as radiometric calibration technique, drone data collection workflows and the particular cameras and spectral filter combinations considered.</p>
<p>The current study reinvestigates the question of the suitability of current commercial infrared converted cameras for use in vegetation condition monitoring. To improve continuity between research, we present a case-study that considers the an equivalent multispectral camera, handheld NDVI sensor and infrared converted camera used by the study of <span class="citation" data-cites="gomes2021">(<a href="#ref-gomes2021" role="doc-biblioref">Gomes et al. 2021</a>)</span>. Our experiment differs however in (1) the radiometric calibration workflow used for the sensors, (2) specific spectral filter used in the infrared converted camera, and (3) the crop type that was captured. Technical specifications of the sensors are more thoroughly described, following <span class="citation" data-cites="pavelka2022">Pavelka, Raeva, and Pavelka (<a href="#ref-pavelka2022" role="doc-biblioref">2022</a>)</span>, and discussed as possible sources of measurement discrepancies between the sensors. By critically evaluating the performance of infrared converted sensors for vegetation monitoring, the agriculture and environmental sciences sectors can make informed decisions about what systems to use, and their potential shortcomings <span class="citation" data-cites="vonbueren2015">(<a href="#ref-vonbueren2015" role="doc-biblioref">Bueren et al. 2015</a>)</span>.</p>
</section>
<section id="methodology" class="level1">
<h1>Methodology</h1>
<section id="study-site" class="level2">
<h2 class="anchored" data-anchor-id="study-site">Study site</h2>
<p>Field surveys were conducted on a commercial crop farm outside Iwamizawa City in Hokkaido prefecture, Japan. Surveys were flown over a potato field (3.4 hectare) and a soybean field (1.9 hectare) <a href="#fig-study_site">Figure&nbsp;1</a>. Hokkaido prefecture is an agriculturally important prefecture in Japan, accounting for 25 % of Japan’s total cultivated area. Farming in this prefecture also consists mostly of commercial farming households, characterised by large farms (13 times larger area per household than other prefectures) that are managed on a full time basis <span class="citation" data-cites="hokkaidodoa2020">(<a href="#ref-hokkaidodoa2020" role="doc-biblioref">Hokkaido DoA 2020</a>)</span>. These characteristics make Hokkaido’s agriculture sector more suitable for adoption of new agricultural technologies like drones <span class="citation" data-cites="swinton2001">(<a href="#ref-swinton2001" role="doc-biblioref">Swinton and Lowenberg-Deboer 2001</a>)</span>. Hokkaido is also the largest producer of potatoes (32.8% share) and soybean (47.7 %) among prefectures in Japan <span class="citation" data-cites="hokkaidodoa2020">(<a href="#ref-hokkaidodoa2020" role="doc-biblioref">Hokkaido DoA 2020</a>)</span>. Two field surveys were conducted, the first one 5 July 2022, when the soy plants were still small, and potato crop was in the vegetative growth stage. The second survey was done on 17 August 2022, approximately 6 weeks after the first. At this time the potato crops had started senescence, and the soy crop was showing strong vegetative growth.</p>
<div id="fig-study_site" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="reporting/images/Iwamisawa%20Study%20Site.png" class="img-fluid figure-img" alt="Map of study site"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Field Study Site: potato and soybean fields, Iwamizawa City, Japan, July 2022</figcaption>
</figure>
</div>
</section>
<section id="data-collection" class="level2">
<h2 class="anchored" data-anchor-id="data-collection">Data Collection</h2>
<p>On the two survey days drone surveys were conducted by 2 drone-camera systems. A summary of the sensor and data collection details are given in <a href="#tbl-survey">Table&nbsp;1</a>. The infrared converted camera used in the study was the Mapir Survey 3W camera with an OCN (orange, cyan, near-infrared) spectral filter <span class="citation" data-cites="mapira">(<a href="#ref-mapira" role="doc-biblioref">Mapir, n.d.b</a>)</span>, however the manufacturer recommends the orange band to be treated as substitute for red in vegetation index derivations. The multispectral sensor considered for comparison was the MicaSense RedEdge-M camera. The camera has 5 sensors that each captures a single spectral band <span class="citation" data-cites="micasenseinc.2023">(<a href="#ref-micasenseinc.2023" role="doc-biblioref">Micasense, Inc. 2023</a>)</span>. The central wavelengths and bandwidths of the respective sensors, are given in <a href="#tbl-survey">Table&nbsp;1</a> and illustrated in <a href="#fig-sensorbands">Figure&nbsp;2</a>. Bandwidths are measured by the Full Width at Half Maximum (FWHM) metric. For the Mapir Survey 3W OCN sensor the manufacturer did not supply FWHM bandwidth, but they do supply the central wavelengths and the filter transmission data <span class="citation" data-cites="mapir2022">(<a href="#ref-mapir2022" role="doc-biblioref">Mapir 2022</a>)</span>, which we used to calculate FWHM with functions from the signal processing library <em>gsignal</em> in R <span class="citation" data-cites="vanboxtelg.j.m.etal.2021">(<a href="#ref-vanboxtelg.j.m.etal.2021" role="doc-biblioref">Van Boxtel, G.J.M., et al. 2021</a>)</span>. The central wavelength for the Trimble handheld NDVI meter considered in this study was supplied by the manufacturer <span class="citation" data-cites="trimbleinc.2022">(<a href="#ref-trimbleinc.2022" role="doc-biblioref">Trimble Inc. 2022</a>)</span>, but not bandwidth information. The MicaSense RedEdge-M camera <span class="citation" data-cites="micasenseinc.2019">(<a href="#ref-micasenseinc.2019" role="doc-biblioref">Micasense, Inc. 2019</a>)</span> was mounted on a DJI Inspire 2 quadcoptor drone. The Mapir sensor was mounted on a DJI Phantom 4 Pro drone. We selected a 50 m target flying height for the Mapir camera to corresponds to the studies by <span class="citation" data-cites="gomes2021">Gomes et al. (<a href="#ref-gomes2021" role="doc-biblioref">2021</a>)</span> and <span class="citation" data-cites="argolodossantos2020">Argolo dos Santos et al. (<a href="#ref-argolodossantos2020" role="doc-biblioref">2020</a>)</span>. However, because the MicaSense sensor has a lower image resolution, the Inspire 2 drone was flown at a lower target height of 40 m. This height was chosen so that ground sampling distance differed less than 5 mm/px between the two sensors. Both camera manufacturers supply tools for calculating sensor ground sampling distance at specified heights. Autonomous flight plans were set using 75% overlap between images. The surveys were conducted between approximately 13:00 and 14:30 local time. Additional details of the respective sensors are described in supplementary file S1.</p>
<div class="cell" data-fig.dpi="300">
<div class="cell-output-display">
<div id="fig-sensorbands" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Manuscript_verbose_files/figure-html/fig-sensorbands-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Central wavelength (points) and bandwidth (FWHM, displayed as error bars) for the bands of the three considered sensors. Bandwidth for the Greenseeker sensor was not supplied.</figcaption>
</figure>
</div>
</div>
</div>
<div id="tbl-survey" class="anchored">
<table class="table">
<caption>Table&nbsp;1: Summary of Drone-camera systems and hand-held sensor used during field surveys. Image band and central wavelength is shown, with bandwidth in brackets, measured by the FWHM metric. Wavelength and bandwidth shown in nanometer (<span class="math inline">\(nm\)</span>).</caption>
<colgroup>
<col style="width: 21%">
<col style="width: 41%">
<col style="width: 24%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>MicaSense RedEdge-M</th>
<th>Mapir Survey3W OCN</th>
<th>Trimble Greenseeker</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Spectral bands<br>
[name, band-center, bandwidth) [<span class="math inline">\(nm\)</span>]</td>
<td>blue 475 (20) + green 560 (20) + red 668 (10)+ NIR 842 (40) + red edge 717 (10)</td>
<td>Orange 615 (45) + Cyan 490 (39) + NIR 808 (54)</td>
<td>NIR 780 + red 660</td>
</tr>
<tr class="even">
<td>Drone platform used</td>
<td>DJI Inspire 2</td>
<td>DJI Phantom 4 Pro</td>
<td>(hand-held)</td>
</tr>
<tr class="odd">
<td>Capturing height (AGL)</td>
<td>40 m</td>
<td>50 m</td>
<td>0.4 m - 0.5 m</td>
</tr>
<tr class="even">
<td>Image overlap</td>
<td>75 (%)</td>
<td>75 (%)</td>
<td>N/A</td>
</tr>
</tbody>
</table>
</div>
<p>Both the camera manufacturers supply ground calibration panels that are used for image calibration during subsequent processing. The respective ground panels were photographed before take-off (<a href="#fig-calibration_panels">Figure&nbsp;3</a>).</p>
<div id="fig-calibration_panels" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="reporting/images/targets_picture-page001.png" class="img-fluid figure-img" alt="calibration panels photographed on ground before flight."></p>
<figcaption class="figure-caption">Figure&nbsp;3: Photos of calibration panels for the MicaSense (A) and Mapir (B) cameras, taken before flight</figcaption>
</figure>
</div>
<p>To compare the drone survey results, we measured NDVI values at sample locations in the two fields using the handheld Trimble Greenseeker NDVI meter <span class="citation" data-cites="trimbleinc.2022">(<a href="#ref-trimbleinc.2022" role="doc-biblioref">Trimble Inc. 2022</a>)</span>. The Trimble Greenseeker uses an active sensor that emits bursts of near-infrared (780 nm) and red (660 nm) light, and then measures the reflectance back to the sensor. The device then calculates an NDVI reading and shows it on an LCD screen. It has a 25 cm field of view when capturing at a height of 60 cm. A further description of the sensor, and comparison with other multispectral sensors is given in <span class="citation" data-cites="pavelka2022">Pavelka, Raeva, and Pavelka (<a href="#ref-pavelka2022" role="doc-biblioref">2022</a>)</span>. To correspond field sample measurements with drone based measurements we placed A4 papers and plastic markers in the field at the sample locations. These markers were visible from the drone images, and could be used to identify the locations to sample NDVI from drone images. Since measurements of NDVI are assumed not to be affected by the type of crop or surface measured, we considered sample locations from the ground, potato field and soybean field together. On the first survey, 12 samples were taken with the Trimble Greenseeker, and on the second day, 26, totalling 38 samples.</p>
<div id="fig-sample_marker" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="reporting/images/target_example-page001.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Example of sample location marker (inside red square), as seen from Mapir camera flown on drone 50m AGL</figcaption>
</figure>
</div>
</section>
<section id="data-calibration-and-processing" class="level2">
<h2 class="anchored" data-anchor-id="data-calibration-and-processing">Data calibration and processing</h2>
<p>The images captured by the two cameras were calibrated to reflectance images, to be used in derivation of the NDVI vegetation index. Mapir supplies official GUI software for radiometric calibration of Mapir camera images, called Mapir Camera Control (MCC), as well as python based processing scripts <span class="citation" data-cites="mapir">(<a href="#ref-mapir" role="doc-biblioref">Mapir, n.d.a</a>)</span>. We used the software to convert RAW images to tiff, apply vignette corrections and convert images values to reflectance. For calibration to reflectance the software used the calibration panel images taken during the field survey (<a href="#fig-calibration_panels">Figure&nbsp;3</a>). The resulting calibrated images were processed into a single georectified orthomosaic using the Windows command line utility for OpenDroneMap <span class="citation" data-cites="opendronemapauthors2020">(<a href="#ref-opendronemapauthors2020" role="doc-biblioref">OpenDroneMap Authors 2020</a>)</span>, an opensource drone image processing software. For the MicaSense RedEdge-M camera, the OpenDroneMap software was used to apply radiometric calibration to reflectance, since the software has built in support for this camera model. The software applies black level, vignetting and row gradient gain/exposure compensation <span class="citation" data-cites="opendronemapauthors2020">(<a href="#ref-opendronemapauthors2020" role="doc-biblioref">OpenDroneMap Authors 2020</a>)</span>. In the software the option was selected to compensate for spectral radiance measured by a down-welling light sensor. The output of this processing was a <em>GeoTIFF</em> orthomosaic with a layer for each of the 5 spectral bands captured by the RedEdge-M camera. For the analysis that required individual reflectance images, the python based image processing utility provided by MicaSense <span class="citation" data-cites="micasenseinc.2022">(<a href="#ref-micasenseinc.2022" role="doc-biblioref">Micasense, Inc. 2022</a>)</span> was used to calibrate the images to reflectance. Further details on the radiometric calibration procedure and orthomosaic generation is described in the Supplementary file S1.</p>
<p>Next, the NDVI index was calculated from the reflectance images using <a href="#eq-ndvi">Equation&nbsp;1</a> given below, where NIR is reflectance in the near-infrared band, and Red is reflectance in the red band. For the three sensors considered, the specific red and near-infrared bands can be seen in <a href="#tbl-survey">Table&nbsp;1</a>. The wavelengths measured by the orange band of the infrared converted camera (center 615 nm) does not overlap with that of the red band of the multispectral camera (668 nm)( <a href="#fig-sensorbands">Figure&nbsp;2</a>). This means that differences in reflectance measured by the sensors do not necessarily relate to differences in the measurement accuracy of the sensor, since differences can be the result of actual variation in the reflectances in the respective bands. But, both sensors are used for vegetation mapping, with the respective red and near-infrared channels used as input for vegetation index maps. Therefore it is important to describe differences in equivalent vegetation index maps derived from the two types of sensors, as well as differences in reflectances measured by individual bands.</p>
<p><span id="eq-ndvi"><span class="math display">\[
NDVI = \frac{NIR-Red}{NIR+Red}
\tag{1}\]</span></span></p>
<p>We used the sample location markers visible in the orthophotos to create a vector point file of the sample locations. A buffer of 10 cm radius was made around each sample point location, and then average NDVI value in the buffer was extracted from the NDVI maps. We used the buffer, since the handheld sensor captures NDVI from about 40 - 50 cm above the plant canopy, and thus captures NDVI for an area. The exact field of view, and thus ground sampling distance of the GreenSeeker sensor was however not known, causing ambiguity about the precise area/plants measured by the sensor. Also, for three sample points in the soybean field, the markers were not visible, so we selected NDVI in the first part of the row that the marker was noted down to be in.</p>
</section>
<section id="data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="data-analysis">Data Analysis</h2>
<p>Three analyses were considered for the acquired data.<br>
Firstly, the NDVI values derived from the three sensors were plotted for each sample location, to show the relationship between the values. To describe the relationship between the NDVI measurement of the sensors, we fit a least-squares Linear model between the drone based measurements, and the handheld sensor based measurement. If the drone-borne cameras and handheld sensor measured NDVI the same across sample points there would be an approximately (1-1) linear relationship between NDVI measurements. So to quantify the difference in NDVI measurements from the two sensors, we considered the linear regression model, where the slope of the model is an indication of difference in the sensor’s sensitivity to NDVI changes — a slope of 1 means NDVI values of the sensors scale the same. The intercept is taken to be an indication of a systematic under or over estimation of NDVI. The model <span class="math inline">\(R^2\)</span> is an indication of whether the assumption of a linear relationship between the sensors’ NDVI values was suitable.</p>
<p>As second analysis, we evaluated the differences between the infrared converted camera’s orange band and the multispectral camera’s red band, and also the differences in measurement between the two sensors’ near-infrared bands. Average reflectance in 10 cm buffers around the field sample location was calculated and shown in scatter plots, and correlation between the two sensors readings were evaluated with pearson’s correlation coefficient. To see the pattern at more locations, we generated 5000 random points with 10 cm buffers in the the study area, and extracted reflectances from the orthomosaics of both survey days. This resulted in 10,000 samples for each sensor. Again scatter plots and pearson correlation are used to evaluate the relationship.</p>
<p>For the third analysis we calculated the difference in reflectance between the Mapir camera’s orange band and the MicaSense’s red-band. We considered a single scene over the potato field, and for each camera used a single image taken of the scene. Individual images were chosen instead of the previously generated orthomosaics since ground sampling distance is higher in individual images, and there are less image distortions than in the orthomosaics generated in the photogrammetry software. The difference in reflectance indicated in what parts of the scene the Mapir Sensor measured higher reflectance than the MicaSense sensor, and where the reflectances were similar. We compared the patterns of reflectance over vegetation and bare-ground pixels in the scene.</p>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<section id="ndvi-at-field-sample-points." class="level2">
<h2 class="anchored" data-anchor-id="ndvi-at-field-sample-points.">NDVI at field sample points.</h2>
<p>We measured NDVI with the three described sensors on two survey days in potato and soybean fields, adding up to 38 sample points. <a href="#fig-result_1">Figure&nbsp;5</a> shows the NDVI values measured by each sensor for the two survey days. The figure shows that the infrared converted sensor reported lower NDVI, compared to the multispectral camera and Trimble Greenseeker handheld NDVI sensor. On both days the multispectral drone camera reported higher NDVI values than the handheld sensor. The specific relationship between the sensor readings was evaluated with a least-squares linear model. The multispectral camera’s NDVI readings scaled almost 1-1 with the handheld sensor (model slope = 0.72, <a href="#tbl-R1">Table&nbsp;2</a>), but the camera’s readings were higher (model intercept = 0.31, <a href="#tbl-R1">Table&nbsp;2</a>). The linear model fit between the handheld and MicaSense sensor was better than the fit between the handheld and Mapir infrared converted sensor <span class="math inline">\(R^2 =\)</span> 0.8 and <span class="math inline">\(R^2=\)</span> 0.38 respectively, <a href="#tbl-R1">Table&nbsp;2</a>).</p>
<div class="cell" data-fig.dpi="300">
<div class="cell-output-display">
<div id="fig-result_1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Manuscript_verbose_files/figure-html/fig-result_1-1.png" class="img-fluid figure-img" width="624"></p>
<figcaption class="figure-caption">Figure&nbsp;5: NDVI values measured by the three sensors (indicated by point colour) at the sample locations for (a) the first field survey (2022/07/05) and (b) second field survey (2022/08/17)</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-R1" class="anchored">
<table class="lightable-classic table table-sm table-striped small" data-quarto-postprocess="true">
<caption>Table&nbsp;2: Linear Model coefficients for model between drone sensor NDVI and handheld NDVI reading</caption>
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">sensor</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">term</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">estimate</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">p.value</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">R2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Mapir Survey 3W</td>
<td style="text-align: left;">intercept</td>
<td style="text-align: right;">-0.0513</td>
<td style="text-align: right;">0.0074</td>
<td style="text-align: right;">0.3755</td>
</tr>
<tr class="even">
<td style="text-align: left;">Mapir Survey 3W</td>
<td style="text-align: left;">slope</td>
<td style="text-align: right;">0.1344</td>
<td style="text-align: right;">0.0000</td>
<td style="text-align: right;">0.3755</td>
</tr>
<tr class="odd">
<td style="text-align: left;">MicaSense RedEdge-M</td>
<td style="text-align: left;">intercept</td>
<td style="text-align: right;">0.3110</td>
<td style="text-align: right;">0.0000</td>
<td style="text-align: right;">0.7970</td>
</tr>
<tr class="even">
<td style="text-align: left;">MicaSense RedEdge-M</td>
<td style="text-align: left;">slope</td>
<td style="text-align: right;">0.7203</td>
<td style="text-align: right;">0.0000</td>
<td style="text-align: right;">0.7970</td>
</tr>
</tbody><tfoot>
<tr class="odd">
<td style="text-align: left; padding: 0;"><sup></sup> n = 38</td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tfoot>

</table>
</div>


</div>
</div>
<p>In <a href="#fig-result_2">Figure&nbsp;6</a> the black diagonal line represents the 1-1 line between drone-based measurements and the handheld sensor’s NDVI measurements for all sample points. The low NDVI measured by the infrared converted camera can be seen by the red line lying below the diagonal line, with a shallow slope (slope = 0.13, <a href="#tbl-R1">Table&nbsp;2</a>) . The multispectral sensor (line in blue) measured NDVI closer to the 1-1 line, but consistently measured higher NDVI than the handheld sensor.</p>
<div class="cell" data-fig.dpi="300">
<div class="cell-output-display">
<div id="fig-result_2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Manuscript_verbose_files/figure-html/fig-result_2-1.png" class="img-fluid figure-img" width="624"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Relationship between NDVI measured by hand-held sensor (x-axis) and the two drone sensors (y-axis) at the 38 sample points measured over the 2 field surveys</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="individual-band-reflectances." class="level2">
<h2 class="anchored" data-anchor-id="individual-band-reflectances.">Individual band reflectances.</h2>
<p>To investigate possible reasons for the discrepancy between NDVI measurements of the sensors, we considered the reflectances from the bands used to calculate the NDVI index. At the locations used for field sampling, the reflectance of the multispectral camera’s red band has weak correlation with that of the infrared converted camera’s equivalent orange band (<span class="math inline">\(r =\)</span> 0.02, <a href="#fig-result_indivband">Figure&nbsp;7</a> a ). This relationship can be more clearly seen by taking reflectance values at 10,000 randomly sampled points from the orthomosaics of the two field survey days (<span class="math inline">\(r =\)</span> -0.03, <a href="#fig-result_indivband">Figure&nbsp;7</a> b). By comparison, the reflectance in the near-infrared bands of the two sensor showed a stronger linear correlation at the field sampled locations (<span class="math inline">\(r =\)</span> 0.28, <a href="#fig-result_indivband">Figure&nbsp;7</a> c) and also if considering the 10,000 randomly sampled points from the orthomosaics (<span class="math inline">\(r =\)</span> 0.87, <a href="#fig-result_indivband">Figure&nbsp;7</a> d).</p>
<div class="cell" data-fig.dpi="300">
<div class="cell-output-display">
<div id="fig-result_indivband" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Manuscript_verbose_files/figure-html/fig-result_indivband-1.png" class="img-fluid figure-img" width="624"></p>
<figcaption class="figure-caption">Figure&nbsp;7: Scatterplots comparing band reflectances, rescaled between 0 and 1, for the infrared converted camera’s orange band (615 nm) and Multispectral camera’s red band (668 nm), taken at (a) the field sampling points, and also at (b) 10,000 randomly sampled points in the orthomosaics. Similarly sub-figures (c) and (d) compare the near-infrared bands measured at the field sampling locations and at 10,000 randomly sampled points respectively.</figcaption>
</figure>
</div>
</div>
</div>
<div id="fig-scene1red" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="reporting/images/scene_1-page001.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;8: The difference in red reflectance captured by the infrared converted and multispectral camera. Shown for a scene over the potato field, with (a) RGB image, (b) the difference between Mapir and MicaSense red channels, (c) Mapir red reflectance, and (d) MicaSense red reflectance</figcaption>
</figure>
</div>
<p>The cause of the difference in reflectance in the red bands is further investigated by considering images captured over the same area of the surveyed potato field that has vegetated and bare-ground pixels (<a href="#fig-scene1red">Figure&nbsp;8</a> a). The infrared converted camera recorded higher reflectance in the orange band (<a href="#fig-scene1red">Figure&nbsp;8</a> c) than the multispectral sensor’s red band (<a href="#fig-scene1red">Figure&nbsp;8</a> d). This can be further seen by taking the difference in reflectance measured by the two sensors (<a href="#fig-scene1red">Figure&nbsp;8</a> b). For bare-ground in the image the measured reflectances were similar, since the difference in reflectance is near to 0 (<a href="#fig-scene1red">Figure&nbsp;8</a> b). However, over the vegetation in the scene there is a difference in the reflectances measured by the sensors of between 0.15 and 0.2. This shows that Mapir’s camera recorder comparatively high ‘red’ reflectances over the vegetation.</p>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>In this case study it was observed that the Mapir Survey3W OCN camera — a commercial infrared converted camera marketed for use in agriculture — consistently measured lower NDVI values in a crop field, compared to a more expensive multi-sensor multispectral camera, and a handheld NDVI meter. This result reflects those of the study by <span class="citation" data-cites="gomes2021">Gomes et al. (<a href="#ref-gomes2021" role="doc-biblioref">2021</a>)</span>, who also observed lower NDVI measurements from a Mapir Survey 3W camera, compared to a MicaSense RedEdge-MX (multispectral camera) and Greenseeker handheld sensor, and also <span class="citation" data-cites="argolodossantos2020">(<a href="#ref-argolodossantos2020" role="doc-biblioref">Argolo dos Santos et al. 2020</a>)</span> who noted Mapir Survey RGNIR measured lower NDVI in a maize field than reported by other studies. A possible explanation for this observation may be that the camera considered in our study overestimated reflectance in the red-band (<a href="#fig-result_indivband">Figure&nbsp;7</a>). The Survey 3W OCN uses a filter with center at 615 nm for the red-band (referred to as ‘orange’ by the manufacturer and elsewhere in this article). This central wavelength is nearer to green wavelengths than common for red bands used in multispectral sensors (<a href="#fig-sensorbands">Figure&nbsp;2</a>). This highlights the fact that vegetation index measurements made by different sensors might not be directly comparable, since vegetation indices like NDVI might be sensitive to the specific band sensitives of different sensors.</p>
<p>An additional reason for the inconsistency in measurement might be because single-sensor RGB cameras that differentiate color channels using a Bayer color filter array have comparatively broad band-sensitivities, if compared to multispectral cameras with separate sensors for each measured band <a href="#fig-sensorbands">Figure&nbsp;2</a>. These two factors – red band wavelength of the spectral filter used in the Survey 3W OCN camera, and the broader band sensitivity of infrared converted cameras in general – may be causing the red-band of the sensor to measure reflectance from the neighboring parts of the spectrum (<a href="#fig-scene1red">Figure&nbsp;8</a>). Specifically, since vegetation also reflects green light, the sensor might be capturing ‘green’ reflectance coming from vegetation, leading to too high readings in the red band, and thus underestimation of NDVI measurements (<a href="#fig-result_1">Figure&nbsp;5</a>). If this is the case, a more comparable vegetation index measurement might be obtained if using infrared converted cameras equipped with spectral filter that has red channel sensitivity centered further away from the green part of the spectrum.</p>
<p>A third possible contributor to the inconsistent measurements may be related to radiometric calibration errors. This could happen if the lighting on the calibration target during its capture did not represent the lighting over the field during the flight because of shadow or cloud. However, the fact that similar results were observed on separate field survey days, and also by <span class="citation" data-cites="gomes2021">Gomes et al. (<a href="#ref-gomes2021" role="doc-biblioref">2021</a>)</span>, who used a different calibration approach, suggests that calibration errors are not the sole cause of the error in the NDVI reading. The study by <span class="citation" data-cites="nijland2014">Nijland et al. (<a href="#ref-nijland2014" role="doc-biblioref">2014</a>)</span> investigated the utility of infrared converted cameras for vegetation monitoring, but they also found inadequate band separation to reduce the accuracy of reflectance measured by these cameras. Based on the findings of this study, filters that have a red-channel at a longer wavelength (further away from the green part of the spectrum) may be more suitable than the Orange (615 nm) filter used for the red-channel in our study.</p>
<p><span class="citation" data-cites="vonbueren2015">Bueren et al. (<a href="#ref-vonbueren2015" role="doc-biblioref">2015</a>)</span> suggests that vegetation indices from RGB and infrared converted cameras are most suitable for simple assessment of vegetation condition over a large area (relative condition). Such monitoring might help farm managers to easily identify specific problem areas that can then be further investigated in-field. However, the results of this study suggest that the infrared converted camera considered might not be suitable to inform quantitative decisions like fertilizer application rate in Variable Rate Application systems <span class="citation" data-cites="alley2011">(<a href="#ref-alley2011" role="doc-biblioref">Alley et al. 2011</a>)</span> or biomass/yield estimation models that are calibrated for accurate NDVI measurements. If infrared converted cameras are indeed mostly suitable for relative condition type measurement, it brings to question whether such converted cameras hold an advantage over unmodified RGB cameras that are generally even cheaper than infrared converted cameras. Regular RGB cameras attached to drones have indeed successfully been used for biomass and yield estimation studies <span class="citation" data-cites="li2018 argolodossantos2020 bendig2014">(<a href="#ref-li2018" role="doc-biblioref">Li et al. 2018</a>; <a href="#ref-argolodossantos2020" role="doc-biblioref">Argolo dos Santos et al. 2020</a>; <a href="#ref-bendig2014" role="doc-biblioref">Bendig et al. 2014</a>)</span>, and for estimating biomass of green algae in the ocean <span class="citation" data-cites="xu2018">(<a href="#ref-xu2018" role="doc-biblioref">Xu et al. 2018</a>)</span>. Some of those studies utilized the fact that RGB cameras capture comparatively high resolution images, which are used to generate three dimensional models of vegetation canopy. This 3-D information, together with visible-band vegetation indices may be sufficient for calculating crop biomass, which in turn can be related to expected yield in some crops, or area covered by eutrophication in water bodies. Visible band indices, although not as sensitive to variation in growth vigor as vegetation indices that incorporate near-infrared bands (e.g., NDVI), are still suitable for distinguishing crops from bare-soil background <span class="citation" data-cites="riehle2020">(<a href="#ref-riehle2020" role="doc-biblioref">Riehle, Reiser, and Griepentrog 2020</a>)</span>, and thus for estimating the area of crop cover. Interestingly, <span class="citation" data-cites="nijland2014">Nijland et al. (<a href="#ref-nijland2014" role="doc-biblioref">2014</a>)</span> observed that an unmodified RGB camera could quantify crop condition more accurately than an infrared converted camera, despite the theoretical advantage of the converted camera to capture infrared wavelengths. <span class="citation" data-cites="fernandez-figueroa2022">Fernandez-Figueroa, Wilson, and Rogers (<a href="#ref-fernandez-figueroa2022" role="doc-biblioref">2022</a>)</span> also tested an infrared converted Mapir Survey 3W for measuring Chl <em>a</em> concentration in ocean algal blooms, and found that the converted camera’s vegetation indices correlated worse to measured Chl <em>a</em> than visible band indices captured by a regular RGB camera.</p>
<p><strong>Conclusion</strong></p>
<p>In conclusion then, our study observed that NDVI measured by an infrared converted did not agree well with a more expensive (multi-sensor) multispectral camera, and so should be used cautiously for quantitative applications like determining fertilizer application rates in crop fields. The inaccuracy might be caused by broad band sensitivities, leading to the red band capturing light from the green wavelengths. This problem might further be exacerbated by the orange (615 nm) filter used for the red band in the Survey 3W OCN camera, which is itself nearer to green wavelengths then most other infrared converted or multispectral drone cameras. Still, this sensor is useful for assessment of comparative vegetation condition to easily identify problem areas. Further work can investigate to what extent infrared-converted camera holds an advantage over unmodified RGB cameras and visible band indices, for applications where only relative condition of vegetation is important.</p>
<section id="code-andor-data-availability" class="level2">
<h2 class="anchored" data-anchor-id="code-andor-data-availability">Code and/or data Availability</h2>
<p>This manuscript was prepared using Quarto <span class="citation" data-cites="allaire2022">(<a href="#ref-allaire2022" role="doc-biblioref">Allaire et al. 2022</a>)</span>, in which executable R scripts used to generate results, and Markdown used to style the text sections are included in a single document. The source Quarto documents and accompanying codes and files are published to a public Github repository to support reproducible research (available online at the url: <a href="https://github.com/StephanLo/drone_sensor_article" class="uri">https://github.com/StephanLo/drone_sensor_article</a>). The software used throughout this analysis is also open-source and publicly available on the internet. The raw field data used for this analysis is not persistently stored in an online repository due to the large data size of the drone imagery. However, it can be shared upon reasonable request to the corresponding author.</p>
</section>
<section id="credit-authorship-contribution-statement" class="level2">
<h2 class="anchored" data-anchor-id="credit-authorship-contribution-statement">CRediT authorship contribution statement</h2>
<p><strong>Albertus S. Louw:</strong> Conceptualization, Methodology, Software, Investigation, Visualization, Writing- Original Draft, Writing- Review &amp; Editing. <strong>Chen Xinyu:</strong> Methodology, Data Curation, Investigation, Software. <strong>Ram Avtar:</strong> Conceptualization, Supervision, Writing- Review &amp; Editing, Funding Acquisition, Resources.</p>
</section>
<section id="funding" class="level2">
<h2 class="anchored" data-anchor-id="funding">Funding</h2>
<p>Funding: This work was supported by Asia Pacific Network for Global Change Research (APN-GCR) grant no. CBA2020-1033-Avtar. The funding source had no role in study design; in the collection, analysis and interpretation of data or writing of the manuscript.</p>
</section>
</section>
<section id="references" class="level1 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-allaire2022" class="csl-entry" role="listitem">
Allaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2022. <em>Quarto</em>. <a href="https://github.com/quarto-dev/quarto-cli">https://github.com/quarto-dev/quarto-cli</a>.
</div>
<div id="ref-alley2011" class="csl-entry" role="listitem">
Alley, Mark, Wade Thomason, David Holshouser, and Gary T Roberson. 2011. <span>“Precision Farming Tools: Variable-Rate Application.”</span> Petersburg, VA.
</div>
<div id="ref-argolodossantos2020" class="csl-entry" role="listitem">
Argolo dos Santos, Robson, Everardo Chartuni Mantovani, Roberto Filgueiras, Elpídio Inácio Fernandes-Filho, Adelaide Cristielle Barbosa da Silva, and Luan Peroni Venancio. 2020. <span>“Actual Evapotranspiration and Biomass of Maize from a Red<span></span>Green-Near-Infrared (RGNIR) Sensor on Board an Unmanned Aerial Vehicle (UAV).”</span> <em>Water</em> 12 (9): 2359. <a href="https://doi.org/10.3390/w12092359">https://doi.org/10.3390/w12092359</a>.
</div>
<div id="ref-barajas2021" class="csl-entry" role="listitem">
Barajas, Jorge, Christian Detweiler, Cailyn Lager, Charles Seaver, Mark Vakarchuk, Justin Henriques, and Jason Forsyth. 2021. <span>“2021 Systems and Information Engineering Design Symposium (SIEDS).”</span> In, 1–5. <a href="https://doi.org/10.1109/SIEDS52267.2021.9483788">https://doi.org/10.1109/SIEDS52267.2021.9483788</a>.
</div>
<div id="ref-bendig2014" class="csl-entry" role="listitem">
Bendig, Juliane, Andreas Bolten, Simon Bennertz, Janis Broscheit, Silas Eichfuss, and Georg Bareth. 2014. <span>“Estimating Biomass of Barley Using Crop Surface Models (CSMs) Derived from UAV-Based RGB Imaging.”</span> <em>Remote Sensing</em> 6 (11): 10395–412. <a href="https://doi.org/10.3390/rs61110395">https://doi.org/10.3390/rs61110395</a>.
</div>
<div id="ref-berra2015" class="csl-entry" role="listitem">
Berra, E., S. Gibson-Poole, A. MacArthur, R. Gaulton, and A. Hamilton. 2015. <span>“Estimation of the Spectral Sensitivity Functions of Un-Modified and Modified Commercial Off-the-Shelf Digital Cameras to Enable Their Use as a Multispectral Imaging System for UAVs.”</span> In. Newcastle University.
</div>
<div id="ref-vonbueren2015" class="csl-entry" role="listitem">
Bueren, S. K. von, A. Burkart, A. Hueni, U. Rascher, M. P. Tuohy, and I. J. Yule. 2015. <span>“Deploying Four Optical UAV-Based Sensors over Grassland: Challenges and Limitations.”</span> <em>Biogeosciences</em> 12 (1): 163–75. <a href="https://doi.org/10.5194/bg-12-163-2015">https://doi.org/10.5194/bg-12-163-2015</a>.
</div>
<div id="ref-burggraaff2019" class="csl-entry" role="listitem">
Burggraaff, Olivier, Norbert Schmidt, Jaime Zamorano, Klaas Pauly, Sergio Pascual, Carlos Tapia, Evangelos Spyrakos, and Frans Snik. 2019. <span>“Standardized Spectral and Radiometric Calibration of Consumer Cameras.”</span> <em>Optics Express</em> 27 (14): 19075–101. <a href="https://doi.org/10.1364/OE.27.019075">https://doi.org/10.1364/OE.27.019075</a>.
</div>
<div id="ref-corti2019" class="csl-entry" role="listitem">
Corti, Martina, Daniele Cavalli, Giovanni Cabassi, Antonio Vigoni, Luigi Degano, and Pietro Marino Gallina. 2019. <span>“Application of a Low-Cost Camera on a UAV to Estimate Maize Nitrogen-Related Variables.”</span> <em>Precision Agriculture</em> 20 (4): 675–96. <a href="https://doi.org/10.1007/s11119-018-9609-y">https://doi.org/10.1007/s11119-018-9609-y</a>.
</div>
<div id="ref-cucho-padin2020" class="csl-entry" role="listitem">
Cucho-Padin, Gonzalo, Hildo Loayza, Susan Palacios, Mario Balcazar, Mariella Carbajal, and Roberto Quiroz. 2020. <span>“Development of Low-Cost Remote Sensing Tools and Methods for Supporting Smallholder Agriculture.”</span> <em>Applied Geomatics</em> 12 (3): 247–63. <a href="https://doi.org/10.1007/s12518-019-00292-5">https://doi.org/10.1007/s12518-019-00292-5</a>.
</div>
<div id="ref-fernandez-figueroa2022" class="csl-entry" role="listitem">
Fernandez-Figueroa, Edna G., Alan E. Wilson, and Stephanie R. Rogers. 2022. <span>“Commercially Available Unoccupied Aerial Systems for Monitoring Harmful Algal Blooms: A Comparative Study.”</span> <em>Limnology and Oceanography: Methods</em> 20 (3): 146–58. <a href="https://doi.org/10.1002/lom3.10477">https://doi.org/10.1002/lom3.10477</a>.
</div>
<div id="ref-fernandez-gallego2019" class="csl-entry" role="listitem">
Fernandez-Gallego, Jose A., Shawn C. Kefauver, Thomas Vatter, Nieves Aparicio Gutiérrez, María Teresa Nieto-Taladriz, and José Luis Araus. 2019. <span>“Low-Cost Assessment of Grain Yield in Durum Wheat Using RGB Images.”</span> <em>European Journal of Agronomy</em> 105 (April): 146–56. <a href="https://doi.org/10.1016/j.eja.2019.02.007">https://doi.org/10.1016/j.eja.2019.02.007</a>.
</div>
<div id="ref-gomes2021" class="csl-entry" role="listitem">
Gomes, Amanda P. A., Daniel M. de Queiroz, Domingos S. M. Valente, Francisco de A. de C. Pinto, and Jorge T. F. Rosas. 2021. <span>“Comparing a Single-Sensor Camera with a Multisensor Camera for Monitoring Coffee Crop Using Unmanned Aerial Vehicles.”</span> <em>Engenharia Agrícola</em> 41 (March): 87–97. <a href="https://doi.org/10.1590/1809-4430-Eng.Agric.v41n1p87-97/2021">https://doi.org/10.1590/1809-4430-Eng.Agric.v41n1p87-97/2021</a>.
</div>
<div id="ref-hafeez2022" class="csl-entry" role="listitem">
Hafeez, Abdul, Mohammed Aslam Husain, S. P. Singh, Anurag Chauhan, Mohd. Tauseef Khan, Navneet Kumar, Abhishek Chauhan, and S. K. Soni. 2022. <span>“Implementation of Drone Technology for Farm Monitoring <span>&amp;</span> Pesticide Spraying: A Review.”</span> <em>Information Processing in Agriculture</em>, February. <a href="https://doi.org/10.1016/j.inpa.2022.02.002">https://doi.org/10.1016/j.inpa.2022.02.002</a>.
</div>
<div id="ref-hokkaidodoa2020" class="csl-entry" role="listitem">
Hokkaido DoA. 2020. <span>“Agriculture in Hokkaido Japan,”</span> June. <a href="https://www.pref.hokkaido.lg.jp/fs/2/3/7/6/7/7/5/_/genjyou_english_0206.pdf">https://www.pref.hokkaido.lg.jp/fs/2/3/7/6/7/7/5/_/genjyou_english_0206.pdf</a>.
</div>
<div id="ref-huang2021" class="csl-entry" role="listitem">
Huang, Sha, Lina Tang, Joseph P. Hupy, Yang Wang, and Guofan Shao. 2021. <span>“A Commentary Review on the Use of Normalized Difference Vegetation Index (NDVI) in the Era of Popular Remote Sensing.”</span> <em>Journal of Forestry Research</em> 32 (1): 1–6. <a href="https://doi.org/10.1007/s11676-020-01155-1">https://doi.org/10.1007/s11676-020-01155-1</a>.
</div>
<div id="ref-lebourgeois2008" class="csl-entry" role="listitem">
Lebourgeois, Valentine, Agnès Bégué, Sylvain Labbé, Benjamin Mallavan, Laurent Prévot, and Bruno Roux. 2008. <span>“Can Commercial Digital Cameras Be Used as Multispectral Sensors? A Crop Monitoring Test.”</span> <em>Sensors</em> 8 (11): 7300–7322. <a href="https://doi.org/10.3390/s8117300">https://doi.org/10.3390/s8117300</a>.
</div>
<div id="ref-li2018" class="csl-entry" role="listitem">
Li, Jiating, Yeyin Shi, Arun-Narenthiran Veeranampalayam-Sivakumar, and Daniel P. Schachtman. 2018. <span>“Elucidating Sorghum Biomass, Nitrogen and Chlorophyll Contents with Spectral and Morphological Traits Derived from Unmanned Aircraft System.”</span> <em>Frontiers in Plant Science</em> 9. <a href="https://www.frontiersin.org/article/10.3389/fpls.2018.01406">https://www.frontiersin.org/article/10.3389/fpls.2018.01406</a>.
</div>
<div id="ref-maes2019" class="csl-entry" role="listitem">
Maes, Wouter H., and Kathy Steppe. 2019. <span>“Perspectives for Remote Sensing with Unmanned Aerial Vehicles in Precision Agriculture.”</span> <em>Trends in Plant Science</em> 24 (2): 152–64. <a href="https://doi.org/10.1016/j.tplants.2018.11.007">https://doi.org/10.1016/j.tplants.2018.11.007</a>.
</div>
<div id="ref-mapir2022" class="csl-entry" role="listitem">
Mapir. 2022. <span>“Filter Transmission Data.”</span> <a href="https://mapir.gitbook.io/kernel2-user-manual/filter-transmission-data">https://mapir.gitbook.io/kernel2-user-manual/filter-transmission-data</a>.
</div>
<div id="ref-mapir" class="csl-entry" role="listitem">
———. n.d.a. <span>“Processing Survey3 Camera Images.”</span> <a href="https://www.mapir.camera/en-gb/pages/processing-survey3-camera-images">https://www.mapir.camera/en-gb/pages/processing-survey3-camera-images</a>.
</div>
<div id="ref-mapira" class="csl-entry" role="listitem">
———. n.d.b. <span>“Survey3W Camera - Orange+Cyan+NIR (OCN, NDVI).”</span> <a href="https://www.mapir.camera/products/survey3w-camera-orange-cyan-nir-ocn-ndvi">https://www.mapir.camera/products/survey3w-camera-orange-cyan-nir-ocn-ndvi</a>.
</div>
<div id="ref-micasenseinc.2019" class="csl-entry" role="listitem">
Micasense, Inc. 2019. <span>“Rededge-MX Dual Camera Imaging System Specifications.”</span> <a href="https://micasense.com/wp-content/uploads/2019/11/Trifold-Dual-Camera-Product-Sheet.pdf">https://micasense.com/wp-content/uploads/2019/11/Trifold-Dual-Camera-Product-Sheet.pdf</a>.
</div>
<div id="ref-micasenseinc.2022" class="csl-entry" role="listitem">
———. 2022. <span>“GitHub - Micasense/Imageprocessing: MicaSense RedEdge and Altum Image Processing Tutorials.”</span> <a href="https://github.com/micasense/imageprocessing">https://github.com/micasense/imageprocessing</a>.
</div>
<div id="ref-micasenseinc.2023" class="csl-entry" role="listitem">
———. 2023. <span>“What Is the Center Wavelength and Bandwidth of Each Filter for MicaSense Sensors?”</span> <a href="https://support.micasense.com/hc/en-us/articles/214878778-What-is-the-center-wavelength-and-bandwidth-of-each-filter-for-MicaSense-sensors-">https://support.micasense.com/hc/en-us/articles/214878778-What-is-the-center-wavelength-and-bandwidth-of-each-filter-for-MicaSense-sensors-</a>.
</div>
<div id="ref-myneni1995" class="csl-entry" role="listitem">
Myneni, Ranga B., Forrest G. Hall, Piers J. Sellers, and Alexander L. Marshak. 1995. <span>“The Interpretation of Spectral Vegetation Indexes.”</span> <em>IEEE Transactions on Geoscience and Remote Sensing</em> 33 (2): 481–86. <a href="https://doi.org/10.1109/TGRS.1995.8746029">https://doi.org/10.1109/TGRS.1995.8746029</a>.
</div>
<div id="ref-nijland2014" class="csl-entry" role="listitem">
Nijland, Wiebe, Rogier de Jong, Steven M. de Jong, Michael A. Wulder, Chris W. Bater, and Nicholas C. Coops. 2014. <span>“Monitoring Plant Condition and Phenology Using Infrared Sensitive Consumer Grade Digital Cameras.”</span> <em>Agricultural and Forest Meteorology</em> 184 (January): 98–106. <a href="https://doi.org/10.1016/j.agrformet.2013.09.007">https://doi.org/10.1016/j.agrformet.2013.09.007</a>.
</div>
<div id="ref-opendronemapauthors2020" class="csl-entry" role="listitem">
OpenDroneMap Authors. 2020. <span>“ODM <span></span> a Command Line Toolkit to Generate Maps, Point Clouds, 3D Models and DEMs from Drone, Balloon or Kite Images.”</span> <a href="https://github.com/OpenDroneMap/ODM">https://github.com/OpenDroneMap/ODM</a>.
</div>
<div id="ref-pavelka2022" class="csl-entry" role="listitem">
Pavelka, Karel, Paulina Raeva, and Karel Pavelka. 2022. <span>“Evaluating the Performance of Airborne and Ground Sensors for Applications in Precision Agriculture: Enhancing the Postprocessing State-of-the-Art Algorithm.”</span> <em>Sensors</em> 22 (19): 7693. <a href="https://doi.org/10.3390/s22197693">https://doi.org/10.3390/s22197693</a>.
</div>
<div id="ref-riehle2020" class="csl-entry" role="listitem">
Riehle, Daniel, David Reiser, and Hans W. Griepentrog. 2020. <span>“Robust Index-Based Semantic Plant/Background Segmentation for RGB- Images.”</span> <em>Computers and Electronics in Agriculture</em> 169 (January): 1–12. <a href="https://doi.org/10.1016/j.compag.2019.105201">https://doi.org/10.1016/j.compag.2019.105201</a>.
</div>
<div id="ref-sheng2021" class="csl-entry" role="listitem">
Sheng, L. Y., A. W. Azhari, and A. H. Ibrahim. 2021. <span>“Unmanned Aerial Vehicle for Eutrophication Process Monitoring in Timah Tasoh Dam, Perlis, Malaysia.”</span> <em>IOP Conference Series: Earth and Environmental Science</em> 646 (1): 012057. <a href="https://doi.org/10.1088/1755-1315/646/1/012057">https://doi.org/10.1088/1755-1315/646/1/012057</a>.
</div>
<div id="ref-swinton2001" class="csl-entry" role="listitem">
Swinton, Scott M., and James Lowenberg-Deboer. 2001. <span>“Global Adoption of Precision Agriculture Technologies: Who, When and Why.”</span> In, 2:557–62. Citeseer. <a href="https://doi.org/10.1.1.469.9339">https://doi.org/10.1.1.469.9339</a>.
</div>
<div id="ref-trimbleinc.2022" class="csl-entry" role="listitem">
Trimble Inc. 2022. <span>“GreenSeeker Handheld Crop Sensor Datasheet.”</span> <a href="https://assets.ctfassets.net/npb3dl1oqqgh/6J7w3BW4Z1yvQwJLedVznc/f34b3dd463cae9b0a696fba78544c7ad/agriculture-greenseeker-handheld-datasheet-en-us.pdf">https://assets.ctfassets.net/npb3dl1oqqgh/6J7w3BW4Z1yvQwJLedVznc/f34b3dd463cae9b0a696fba78544c7ad/agriculture-greenseeker-handheld-datasheet-en-us.pdf</a>.
</div>
<div id="ref-vanboxtelg.j.m.etal.2021" class="csl-entry" role="listitem">
Van Boxtel, G.J.M., et al. 2021. <em>Gsignal: Signal Processing</em>. <a href="https://github.com/gjmvanboxtel/gsignal">https://github.com/gjmvanboxtel/gsignal</a>.
</div>
<div id="ref-wang2020" class="csl-entry" role="listitem">
Wang, Wenjin, and Albertus C. den Brinker. 2020. <span>“Modified RGB Cameras for Infrared Remote-PPG.”</span> <em>IEEE Transactions on Biomedical Engineering</em> 67 (10): 2893–2904. <a href="https://doi.org/10.1109/TBME.2020.2973313">https://doi.org/10.1109/TBME.2020.2973313</a>.
</div>
<div id="ref-xu2018" class="csl-entry" role="listitem">
Xu, Fuxiang, Zhiqiang Gao, Xiaopeng Jiang, Weitao Shang, Jicai Ning, Debin Song, and Jinquan Ai. 2018. <span>“A UAV and S2A Data-Based Estimation of the Initial Biomass of Green Algae in the South Yellow Sea.”</span> <em>Marine Pollution Bulletin</em> 128 (March): 408–14. <a href="https://doi.org/10.1016/j.marpolbul.2018.01.061">https://doi.org/10.1016/j.marpolbul.2018.01.061</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>