---
title: "On the accuracy of infrared-converted drone cameras for use in vegetation and environmental monitoring"
author:
  - name: Albertus S. Louw
    email: jasahovercar@gmail.com
    affiliations:
      - name: Graduate School of Environmental Science, Hokkaido University
  - name: Chen Xinyu
    affiliations:
    - name: Graduate School of Environmental Science, Hokkaido University
  - name: Ram Avtar
    affiliations:
    - name: Graduate School of Environmental Science, Hokkaido University
execute: 
  cache: true
knitr: 
  opts_chunk: 
    echo: FALSE
prefer-html: true
format:
  docx:
    page-width: 8.3
    prefer-html: true
    fig-dpi: 300
    number-sections: false
    reference-doc: Asian_Agriculture-reference-doc.docx
    csl: apa.csl
  pdf:
    documentclass: article
    margin-left: 25mm
    margin-right: 25mm
    margin-top: 25mm
    margin-bottom: 25mm
    number-sections: true
    keep-tex: true
    fig-pos: "h"
#  html:
#    theme: flatly
#    reference-location: margin
crossref:
   fig-title: '**Fig.**'     # (default is "Figure")
   fig-prefix: '**Fig.**'
   tbl-title: '**Table**'     # (default is "Table")
   title-delim: ""     # (default is ":")
   #labels: '**alpha**'
editor: visual
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

# Abstract

Drones equipped with cameras sensitive to near-infrared wavelengths are increasingly being used in environmental assessment studies and in agriculture. These cameras are sensitive to vegetation cover, extent of euthrophication in water bodies, and aspects of crops, such as growth vigour, biomass and potential yield. Single-sensor ('RGB') cameras with modified spectral filters that allow for capturing near-infrared wavelengths offer a low-cost alternative to multi-sensor multispectral cameras or drone-borne spectrometers. However, some studies point to lower accuracy in measurements by such infrared converted sensors. So, to what extent can infrared converted cameras be used to quantify vegetation condition? This study compared vegetation index measurements (NDVI) from an infrared converted camera to measurements by a multispectral camera and a handheld NDVI meter, captured over soybean and potato fields. It was observed that infrared converted camera derived NDVI was consistently lower over crop than multispectral and handheld based measurements. However, correlation between the sensor values were high (r = 0.95, r = 0.87 for respective survey days). This suggests that the infrared converted sensor is valuable for relative assessment of vegetation status across a farm. Based on the result of this study, however, caution should be taken when using infrared converted camera for quantitative applications, such as creating fertiliser prescription rates. We discuss possible reasons for the lower vegetation index measurements observed, noting overestimation of reflectance in the red band, but underestimation in the near-infrared band, leading to low NDVI values.

keywords: modified RGB camera, drone remote sensing, multispectral drone sensor comparison, UAV environmental monitoring

# Introduction

Drones equipped with cameras are increasingly being used in environmental assessment studies and agriculture. For example, such drone-camera systems have recently been used to monitor ocean algal blooms [@fernandez-figueroa2022] and wetland inundation and vegetation change in an estuarine reserve [@dehm]. Drone cameras are also used to measure different aspects of crops, such as growth vigour, biomass and water-stress [@hafeez2022]. Specialized cameras for vegetation monitoring often have a sensor sensitive to wavelengths in the near-infrared part of the EM-spectrum. Healthy photosynthesising vegetation shows high reflectance in near-infrared wavelengths, but comparatively low reflectance in the red part of the spectrum [@myneni1995]. So, if red and near-infrared wavelengths are recorder by a drone sensor, the condition or growth vigour of vegetation can be estimated with vegetation indices such as the Normalised Difference Vegetation Index, or NDVI [@huang2021]. This index ranges from -1 to 1, with higher values interpreted as vegetation with higher growth vigour. Vegetation indices like NDVI are also being applied to estimate euthrophication in waterbodies [@barajas2021; @sheng2021] , and to indirectly quantify the condition of crops and its variation spatially and over time. Such drone-camera systems offer a comparatively low-cost method to capture image data for wide areas, and allows data to be spatially referenced so that it can be overlaid with other sources of spatially explicit data. Different drone camera sensors however have different price-points and characteristics that may influence the quality of measurements [@nijland2014]. Thus, as the use of these technologies scale in agriculture and environmental studies, it is all the more necessary to evaluate the measurement bias or limitations of different drone sensor types.

There are different ways in which cameras are designed to capture near-infrared wavelengths [@maes2019]. One option is that the camera has a independent imaging sensor and lens for each wavelength band. The second option is that a single sensor camera employing a Bayer Color filter arrat to capture 3 color bands -- often referred to as red-green-blue (RGB) cameras -- is modified to become sensitive to light in the near-infrared spectrum [@lebourgeois2008]. In this article these are referred to as infrared converted cameras [@nijland2014], although they are also referred to as modified RGB [@lebourgeois2008; @wang2020] or modified multispectral cameras [@fernandez-figueroa2022]. Infrared converted cameras work by removing the filter which blocks NIR light from entering the sensor, and then substituting one of the RGB camera's bands for the NIR band. For example, instead of Red-Green-Blue, the camera becomes sensitive to Red-Green-NIR. The single sensor infrared converted cameras are cheaper (by order of magnitude) than multispectral cameras with multiple sensors. They thus pose an attractive alternative, especially in cases where 'proper' multispectral cameras are considered prohibitively expensive. Several studies highlight the value of lowering the cost of technologies that can support environmental monitoring [@fernandez-figueroa2022] and agriculture [@fernandez-gallego2019; @cucho-padin2020; @corti2019].

Infrared converted cameras are an appealing option for drone agriculture remote sensing because of their comparative low-cost and ability to capture near-infrared wavelengths. but it is necessary to verify the accuracy of spectral measurements made by these sensors. Despite being used in studies [@lebourgeois2008; @argolodossantos2020], some authors reported lower measurement accuracy for infrared converted cameras, if compared to multi-sensor cameras or spectroscopes [@vonbueren2015; @gomes2021; @nijland2014]. This might partially be because the bands captured by single-sensor RGB camera is usually sensitive to light outside of the target wavelengths, and so measurements in specific band may be polluted by light in other parts of the spectrum [@burggraaff2019; @berra2015]. This means for example that a modified RGB camera may report incorrect values for a specific band, because the sensor is also capturing light from the neighbouring bands.

Before such infrared converted cameras can be recommended for quantitative environmental monitoring studies, or operational use on farms, it is important to verify that spectral measurements and vegetation indices derived from the infrared converted camera correspond well to measurements made by multispectral cameras, or hand-held spectrometers. The study by @gomes2021 investigated this dynamic for the Mapir Survey3W commercial infrared converted camera, by comparing it to the multispectral MicaSense RedEdge-MX camera. The study calculated the vegetation index NDVI of a coffee plantation using both cameras, as well as a handheld NDVI sensor. It was observed that NDVI measurements made by the infrared converted camera were consistently lower, if compared to the multispectral camera and handheld NDVI sensor. This finding may have important repercussions for the operational use of such infrared converted cameras. Since measurements made such as in the study by @Gomes2021 may be sensitive to multiple factors like light exposure at the time of meaurement, time of the day, radiometric calibration errors, and camera settings. Thus, to strengthen the validity of these findings follow up studies can conduct similar experiments, while varying components such as radiometric calibration technique, drone data collection workflows and the particular cameras and filter combinations considered.

The current study reinvestigates the question of the suitability of current commercial infrared converted cameras for use in vegetation condition monitoring. To improve continuity between research, we present a case-study that considers the same multispectral camera, handheld NDVI sensor and infrared converted camera used by the study of [@gomes2021]. Our experiment differs however in the calibration technique used, specific spectral filter used in the infrared converted camera, and also the crop type that was captured. By critically evaluating the performance of infrared converted sensors for vegetation monitoring, the agriculture and environmental sciences sectors can make informed decisions about what systems to use, and their potential shortcomings [@vonbueren2015].

# Methodology

```{r 1_env_setup, include=FALSE}
# Setup environment
library(terra)
library(sf)
library(data.table)
library(broom)
library(kableExtra)
library(ggplot2)
library(tidyterra)
library(ggthemes)
library(patchwork)
theme_set(theme_clean())

```

```{r SensorCharacteristics}
library(gsignal)
mapir_filter <- openxlsx::read.xlsx("data/Sensor_data/F490-615-808.xlsx", colNames = FALSE) |> as.data.table()
names(mapir_filter) <- c("Wavelength", "Transmission")

mapir_filter <- mapir_filter[complete.cases(mapir_filter),] # remove NA rows

# Divide into 3 bands: (select values less than each 'threshold' )
wave_max <- max(mapir_filter$Wavelength)
wave_min <- min(mapir_filter$Wavelength)

filter_cutpoint <- c(wave_min,550,720,wave_max)
band_names <- c("F490", "F615", "F808")
mapir_filter[,band := cut(Wavelength, breaks = filter_cutpoint, labels = band_names, include.lowest = TRUE)]

## work out max for each band:
f490 <- mapir_filter[band == "F490",]
f615 <- mapir_filter[band == "F615",]
f808 <- mapir_filter[band == "F808",]

## work out fwhm for each:
fwhm_490 <- fwhm(x = f490$Wavelength,
                 y = f490$Transmission)

fwhm_615 <- fwhm(x = f615$Wavelength,
                 y = f615$Transmission)

fwhm_808 <- fwhm(x = f808$Wavelength,
                 y = f808$Transmission)


# ggplot(mapir_filter, aes(x = Wavelength, y = Transmission)) + 
#   geom_smooth(method = "loess", span = 0.01, se = FALSE)+
#   geom_point(size = 1.5) + 
#   
#   scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) 
  
# make our own table
sensor_data <- data.table(sensor = c(rep("RedEdge-M",5),
                                     rep("Survey 3W OCN",3),
                                     rep("Greenseeker",2)),
                          band_name = c("Blue", "Green", "Red", "Red Edge", "NIR",
                                        "Cyan", "Orange", "NIR",
                                        "NIR", "Red"),
                          band_centre = c(475,560,668,717,840,
                                          490, 615, 808,
                                          780, 660),
                          fwhm = c(20,20,10,10,40,39,45,54,NA,NA))

sensor_data[,c("b_min","b_max") := .(band_centre -fwhm/2, band_centre + fwhm/2  )]

openxlsx::write.xlsx(sensor_data ,file = "reporting/supplementary/S1_sensorbands.xlsx")


Sensor_plots <- ggplot( sensor_data, aes(x = band_centre, 
                         y = sensor, 
                         color = sensor, 
                         fill = sensor,
                         xmin = b_min,
                         xmax = b_max)) +
  geom_point(shape = 21, size = 3) +
  #scale_colour_manual(values = rep("black", nrow(sensor_data))) +
  geom_errorbarh(height = 0.3) + 
  geom_text(aes(label = band_name),colour = "black", vjust = -1.5, size = 3)+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  labs(x = "Wavelength (nm)",
       y = "Sensor",
       color = "Sensor",
       fill = "Sensor")

#Sensor_plots
# target size: approx 8.385417 2.875000 (inches)
```

## Study site

Field surveys were conducted on a commercial crop farm outside Iwamizawa City in Hokkaido prefecture, Japan. Surveys were flown over a potato field (33,984 $m^2$ ) and a soybean field (18,898 $m^2$ ) @fig-study_site. Hokkaido prefecture is an agriculturally important prefecture in Japan, accounting for 25 % of Japan's total cultivated area. Farming in this prefecture also consists mostly of commercial farming households, characterised by large farms (13 times larger area per household than other prefectures) that are managed on a full time basis [@hokkaidodoa2020]. These characteristics make Hokkaido's agriculture sector more suitable for adoption of new agricultural technologies like drones [@swinton2001]. Hokkaido is also the largest producer of potatoes (32.8% share) and soybean (47.7 %) among prefectures in Japan [@hokkaidodoa2020]. Two field surveys were conducted, the first one 5 July 2022, when the soy plants were still small, and potato crop was in the vegetative growth stage. The second survey was done on 17 August 2022, approximately 6 weeks after the first. At this time the potato crops had started senescence, and the soy crop was showing strong vegetative growth.

![Field Study Site: potato and soybean fields, Iwamizawa City, Japan, July 2022](reporting/images/Iwamisawa%20Study%20Site.png){#fig-study_site fig-alt="Map of study site"}

## Data Collection

On the two survey days drone surveys were conducted by 2 drone-camera systems. A summary of the field-survey details are given in @tbl-survey. The MicaSense RedEdge-M camera [@micasenseinc.2019] was mounted on a DJI Inspire 2 quadcoptor drone. The infrared converted camera used in the study was the Mapir Survey 3W camera with OCN (orange, cyan, near-infrared) spectral filter [@mapira], however the manufacturer recommends the orange band to be treated as substitute for red in vegetation index derivations. The central wavelength and bandwidth for each band of the respective sensors, is illustrated in @fig-sensorbands. The band centers and bandwidths as measured by the Full Width at Half Maximum (FWHM) metric was provided by MicaSense for the RedEdge-M sensor [@micasenseinc.2023]. For the Mapir Survey 3W OCN sensor the manufacturer supplied the central wavelengths and the filter transmission data [@mapir2022], which we used to calculate FWHM with functions from the signal processing library *gsignal* in R [@vanboxtelg.j.m.etal.2021]. The central wavelength for Trimble handheld NDVI meter considered in this study was supplied by the manufacturer [@trimbleinc.2022], but not bandwidth information. Band centers and bandwidths for the sensors is supplied in supplementary table S1. The Mapir sensor was flown on a DJI Phantom 4 Pro drone. The Inspire 2 surveys were flown at 40 m Above Ground Level (AGL), and the Phantom 4 flights were done at 50 m AGL. We selected a 50 m target flying height for the Mapir camera to corresponds to the studies by @gomes2021 and @argolodossantos2020. However because the MicaSense sensor has a lower image resolution we flew the MicaSense sensor at the lower target height of 40 m. This height was chosen so that ground sampling distance differed less than 5 mm/px between the two sensors. Both camera manufacturers supply tools for calculating sensor ground sampling distance at specified heights. Autonomous flight plans were set using 75% overlap between images. The surveys were conducted between approximately 13:00 and 14:30 local time.

```{r}
#| label: fig-sensorbands
#| fig-cap: Central Wavelength (points) and bandwidth (full width at half maximum, displayed as error bars) for the bands of the three considered sensors. Bandwidth for Greenseeker sensor was not supplied. 
#| fig.height: 3 
#| fig.width: 8
#| fig-dpi: 300
Sensor_plots

```

| Property                           | MicaSense RedEdge-MX                                             | Mapir Survey3W OCN                    | Trimble Greenseeker   |
|-----------------|-----------------------|-----------------|-----------------|
| Spectral bands (band centre, $nm$) | blue (475) + green (560) + red (668)+ NIR (842) + red edge (717) | Orange (615) + Cyan (490) + NIR (808) | NIR (780) + red (660) |
| Drone platform used                | DJI Inspire 2                                                    | DJI Phantom 4 Pro                     | (hand-held)           |
| Capturing height (AGL)             | 40 m                                                             | 50 m                                  | 0.4 m - 0.5 m         |
| Image overlap                      | 75 (%)                                                           | 75 (%)                                | N/A                   |
| Survey local time                  | 13:00 - 14:30                                                    | 13:00 - 14:30                         | 13:00 - 14:30         |

: Summary of Drone-camera systems and hand-held sensor used during field surveys. {#tbl-survey}

Both the camera manufacturers supply ground calibration panels that are used for image calibration during subsequent processing. The respective ground panels were photographed before take-off @fig-calibration_panels.

![Photos of calibration panels for the MicaSense (A) and Mapir (B) cameras, taken before flight](reporting/images/targets_picture-page001.png){#fig-calibration_panels fig-alt="calibration panels photographed on ground before flight."}

To compare the drone survey results, we measured NDVI values at sample locations in the two fields using the handheld Trimble Greenseeker NDVI meter [@trimbleinc.2022]. The Trimble Greenseeker uses an active sensor that emits bursts of near-infrared (780 nm) and red (660 nm) light, and then measures the reflectance back to the sensor. The device then calculates an NDVI reading and shows it on an LCD screen. It has a 25 cm field of view when capturing at a height of 60 cm. To correspond field sample measurements with drone based measurements we placed A4 papers and plastic markers in the field at the sample locations. These markers were visible from the drone images, and could be used to identify the locations to sample NDVI from drone images. Since measurements of NDVI are assumed not to be affected by the type of crop or surface measured, we considered sample locations from the ground, potato field and soybean field together. On the first survey, 12 samples were taken with the Trimble Greenseeker, and on the second day, 26, totalling 38 samples.

![Example of sample location marker (inside red square), as seen from Mapir camera flown on drone 50m AGL](reporting/images/target_example-page001.png){#fig-sample_marker}

## Data calibration and processing

The images captured by the two cameras were calibrated to reflectance images, to be used in derivation of the NDVI vegetation index. Mapir supplies official GUI software for radiometric calibration of Mapir camera images, called Mapir Camera Control (MCC), as well as python based processing scripts [@mapir]. We used the software to convert RAW images to tiff, apply vignette corrections and convert images values to reflectance. For calibration to reflectance the software used the calibration panel images taken during the field survey @fig-calibration_panels. The resulting calibrated images were processed into a single georectified orthomosaic using the Windows command line utility for OpenDroneMap [@opendronemapauthors2020], an opensource drone image processing software. For the MicaSense RedEdge-M camera, the OpenDroneMap software was used to apply radiometric calibration to reflectance, since the software has built in support for this camera model. The software applies black level, vignetting and row gradient gain/exposure compensation [@opendronemapauthors2020]. In the software the option was selected to compensate for spectral radiance measured by a down-welling light sensor. The output of this processing was a *GeoTIFF* orthomosaic with a layer for each of the 5 spectral bands captured by the RedEdge-MX camera. However, for some areas in the field the resulting orthomosaic was blurred, and had lower resolution then the rest of the image. To account for this, we selected individual images of the sample locations that were in the blurred part of the orthomosaic. These individual images were also radiometrically calibrated, and then georectified to overlap with the orthomosaic. To radiometrically calibrate the individual RedEdge-M images, we used the python based image processing utility provided by MicaSense [@micasenseinc.2022], instead of the OpenDroneMap utility, since it is not suitable for processing only individual images.

Next, the NDVI index was calculated from the reflectance images using @eq-ndvi given below, where NIR is reflectance in the near-infrared band, and Red is reflectance in the red band. For the two cameras considered, the specific red and near-infrared bands can be seen in @tbl-survey.

$$
NDVI = \frac{NIR-Red}{NIR+Red}
$$ {#eq-ndvi}

We used the sample location markers visible in the orthophoto to create vector point file of the sample locations. A buffer of 10 cm radius was made around each sample point location, and then average NDVI value in the buffer was extracted from the NDVI maps. We used the buffer, since the handheld sensor captures NDVI from about 40 - 50 cm above the plant canopy, and thus captures NDVI for an area. The exact field of view, and thus ground sampling distance of the GreenSeeker sensor was however not known, causing ambiguity about the precise area/plants measured by the sensor. Also, for three sample points in the soybean field, the markers were not visible, so we selected NDVI in the first part of the row that the marker was noted down to be in.

## Data Analysis

Three analyses were considered for the acquired data.\
Firstly, the NDVI values measured by the three sensors were plotted for each sample location, to show the relationship between the values. Next to quantify the relationship, we fit a least-squares Linear model between the drone based measurements, and the handheld sensor based measurement. If the sensors measured accurately there should be an approximately (1-1) relationship between NDVI measurements of the three sensors. The slope of the model is an indication of difference in the sensor's sensitivity to NDVI changes --- a slope of 1 means NDVI values of the sensors scale the same. The intercept is taken to be an indication of a systematic under or over estimation of NDVI. The model $R^2$ is an indication of whether the assumption of a linear relationship between the sensors' NDVI values was suitable.

For a second analysis, we generated 5000 random points in the the study area, and extracted NDVI from a 10 cm buffer around the point. This was to extend the first analysis by showing a clearer relationship between NDVI values captured by the multispectral and infrared converted cameras. The correlation between the values from two the sensors are calculated, and the points are plotted.

For the third analysis, we generated histograms of the reflectance from the red and near infra-red bands of individual images over the same portion of crops in the field, to understand how the difference in NDVI may be caused by differences in the reflectance measurements in the different bands.

```{r}
# Starting with Orthophoto of study site.
# compare Red and NIR band values at each sample point.


mapir_names <- c("map_Orange", "map_Cyan", "map_NIR")
mica_names <- c("mic_Blue", "mic_Green", "mic_Red", "mic_NIR", "mic_RedEdge")


# load rasters
mapir <- list.files(path = "data/georeferenceddata/for_datum_JGD2011",
                      pattern = "mapir",
                      full.names = TRUE)

mica <- list.files(path = "data/georeferenceddata/for_datum_JGD2011",
                    pattern = "mica",
                    full.names = TRUE)


# Read in files ----
# paths
point_path <- c("data/vectordata/targetsamples_0705.gpkg", "data/vectordata/targetsamples0817.gpkg")

#some of the rasters are in 16bit int and other is in reflectance (decimal up to 1)
bit16 <- 2^16 - 1


## Rasters
mapir_d1_raw <- rast(mapir[1]) |> terra::subset(1:3)
mapir_d2_raw <- rast(mapir[2]) |> terra::subset(1:3)

mapir_d1 <- mapir_d1_raw/bit16
mapir_d2 <- mapir_d2_raw/bit16

names(mapir_d1) <- mapir_names
names(mapir_d2) <- mapir_names

# mica already in reflectance.
mica_d1 <- rast(mica[1]) |> terra::subset(1:5)
mica_d2 <- rast(mica[2]) |> terra::subset(1:5)

names(mica_d1) <- mica_names
names(mica_d2) <- mica_names


# point locations 
samples_d1 <- vect(point_path[1])
samples_d1 <- samples_d1[order(samples_d1$no),] # make sure the row order is according to the no variable.

samples_d2 <- vect(point_path[2])
samples_d2 <- samples_d2[order(samples_d2$no),] # make sure the row order is 


# Make buffer around point. Of 10 cm radius (0.1 meter)
buf_d1 <- buffer(samples_d1, width = 0.1) 
buf_d2 <- buffer(samples_d2, width = 0.1) 

# Extract values at samples.
pv_mapir_d1 <- extract(x = mapir_d1,
                     y = buf_d1[6:14,],
                     fun = mean)


pv_mica_d1 <- extract(x = mica_d1,
                     y = buf_d1[6:14,],
                     fun = mean)

# and for the second day:

pv_mapir_d2 <- extract(x = mapir_d2,
                     y = buf_d2,
                     fun = mean)

pv_mica_d2 <- extract(x = mica_d2,
                     y = buf_d2,
                     fun = mean)



# ON the first day the image quality was bad for some parts of orthophoto, and this resulted in outlier results in the analysis.
# To mitigate this, I calibraed individual images from the survey flight that covered the relevant sample sites, 
# and then georeferenced the images over the rest of the orthophoto. Next, for those scenes I extracted NDVI values from the
# # individual images, rather than the blurred orthophoto.
# # Individual Images --

# For individual images
mapir_im1 <- rast("data/individualimages/together-calibrated/1_mapir_modified.tif") /bit16
mica_im1 <- rast("data/individualimages/together-calibrated/1_mica_modified.tif")/bit16 

mapir_im2 <- rast("data/individualimages/together-calibrated/2_mapir_modified.tif")/bit16
mica_im2 <- rast("data/individualimages/together-calibrated/2_mica_modified.tif")/bit16 
 

# Names
names(mapir_im1) <- mapir_names
names(mapir_im2) <- mapir_names

names(mica_im1) <- mica_names
names(mica_im2) <- mica_names


# extract the points from individual images. 
mapir_indiv <- rbind(
  extract(
    x = mapir_im1,
    y = buf_d1[1:2, ],
    fun = mean
  ),
  extract(
    x = mapir_im2,
    y = buf_d1[3:5, ],
    fun = mean
  )
)

mica_indiv <- rbind(
  extract(
    x = mica_im1,
    y = buf_d1[1:2, ],
    fun = mean
  ),
  extract(
    x = mica_im2,
    y = buf_d1[3:5, ],
    fun = mean
  )
)


# combine the observations from the individual images and the orthophoto.
samples_mapir <- rbind(mapir_indiv,
                    pv_mapir_d1,
                    pv_mapir_d2)

samples_mapir$ID <- 1:nrow(samples_mapir)

samples_mapir$sensor <- "mapir"


samples_mica <- rbind(mica_indiv,
                    pv_mica_d1,
                    pv_mica_d2)

samples_mica$ID <- 1:nrow(samples_mica)

samples_mica$sensor <- "mica"


setDT(samples_mapir)
setDT(samples_mica)

# make to long.

mapir_long <- melt(samples_mapir, 
                   id.vars = c("ID", "sensor"),
                   variable.name = "band",
                   value.name = "value")

mica_long <- melt(samples_mica, 
                   id.vars = c("ID", "sensor"),
                   variable.name = "band",
                   value.name = "value")


field_samples <- rbind(mapir_long,mica_long)


# bit wider
field_samples_wider <- dcast(data = field_samples,
                             formula = ID ~ band,
                             value.var = "value")



## Make graphs here for now, but move to results section later if to be included.

ggplot(field_samples_wider[ID > 5,], aes(x = map_NIR, y = mic_NIR) ) + geom_point()

ggplot(field_samples_wider[ID > 5,], aes(x = map_Orange, y = mic_Red) ) + geom_point()

# correlation between band values?
samples_corr <- field_samples_wider[ID>5, .(cor(map_Orange, mic_Red), cor(map_NIR, mic_NIR))]

```

```{r A1_field_sample_points, include=FALSE}
# Script: NDVI point analysis: paper targets
# author: AS Louw
# enquiry: jasahovercar@gmail.com


# Read in files ----
# paths
point_path <- c("data/vectordata/targetsamples_0705.gpkg", "data/vectordata/targetsamples0817.gpkg")

survey_path <- "data/Yao_farm_surveys_2022.xlsx" 
survey_sheet <- c("20220705_trimble_NDVI", "20220817_trimble_NDVI") #sheet name
r_path <- list.files(path = "data/ndvimaps/",
                      full.names = TRUE,
                      pattern = ".tif")

r_names <- list.files(path = "data/ndvimaps/",
                      pattern = ".tif") |> (\(x) substr(x,start = 1, stop = nchar(x)-4))() # this takes the name but drops the .tif extension
# more info on the syntax https://www.r-bloggers.com/2021/05/the-new-r-pipe/

# Data Preparation ----
## CAUTION: there is a lot off messy merging and manipulation going on in the code below. Try tidy up later.

# DAY 1 
## if first date, use path index 1&3 and surveysheet 1, if second date, 2&4 and survey sheet 2, 
vi_raster <- rast(r_path[c(1,3)])  
names(vi_raster) <- c("sensor_1", "sensor_2")

rpoints_vec <- vect(point_path[1])
rpoints_vec <- rpoints_vec[order(rpoints_vec$no),] # make sure the row order is according to the no variable.

survey_df <- readxl::read_excel(path = survey_path,
                                sheet = survey_sheet[1])

# Make buffer around point. Of 10 cm radius (0.1 meter)
p_buf <- buffer(rpoints_vec, width = 0.1) 

# ON the first day the image quality was bad for some parts of orthophoto, and this resulted in outlier results in the analysis.
# To mitigate this, I calibraed individual images from the survey flight that covered the relevant sample sites, 
# and then georeferenced the images over the rest of the orthophoto. Next, for those scenes I extracted NDVI values from the
# individual images, rather than the blurred orthophoto.
# Individual Images --
sensor_1_scene_1 <- rast("data/individualimages/ndvi/mapir_scene_1.tif")
sensor_2_scene_1 <- rast("data/individualimages/ndvi/mica_scene_1.tif") |> resample(sensor_1_scene_1)

sensor_1_scene_2 <- rast("data/individualimages/ndvi/mapir_scene_2.tif") 
sensor_2_scene_2 <- rast("data/individualimages/ndvi/mica_scene_2.tif") |> resample(sensor_1_scene_2)

# Scene 1 has points 1 and 2
points_s1 <- 
  extract(
    x = c(sensor_1_scene_1,sensor_2_scene_1),
    y = p_buf[1:2, ],
    fun = mean
  )
points_s2 <- extract(
  x =c(sensor_1_scene_2,sensor_2_scene_2),
  y = p_buf[3:5, ],
  fun = mean
)
names(points_s1) <- c("ID", "sensor_1", "sensor_2")
names(points_s2) <- c("ID", "sensor_1", "sensor_2")

#extract ndvi at the points in the orthophoto:
ortho_points <- extract(x = vi_raster,
                     y = p_buf[6:14,],
                     fun = mean)
# combine the observations from the individual images and the orthophoto.
d1_points <- rbind(points_s1,
                   points_s2,
                   ortho_points)
d1_points$ID <- 1:nrow(d1_points)

d1_points <- merge(d1_points, values(rpoints_vec),by.x = "ID", by.y = "no")
d1_points$day <- "survey 1"
d1_all <- merge(d1_points, survey_df[,1:4], by.x = "ID", by.y = "ID")

# DAY 2 
## For the second day, the orthophoto quality was better, and we use it directly.
## if first date, use path index 1&3 and surveysheet 1, if second date, 2&4 and survey sheet 2, 

#read in data
vi_raster <- rast(r_path[c(2,4)])  
names(vi_raster) <- c("sensor_1", "sensor_2")
rpoints_vec <- vect(point_path[2])
rpoints_vec <- rpoints_vec[order(rpoints_vec$no),] # make sure the row order is according to the no variable.
survey_df <- readxl::read_excel(path = survey_path,
                                sheet = survey_sheet[2])
# Make buffer around point. Of 10 cm radius (0.1 meter)
p_buf <- buffer(rpoints_vec, width = 0.1) 
#extract average ndvi from the sample points
d2_points <- extract(x = vi_raster,
                     y = p_buf,
                     fun = mean)

d2_points <- merge(d2_points, values(rpoints_vec),by.x = "ID", by.y = "no")
d2_points$day <- "survey 2"
d2_all <- merge(d2_points, survey_df, by.x = "ID", by.y = "ID")


all_df <- rbind(d1_all,d2_all) # COmbine DAY 1 and DAY 2
names(all_df)[names(all_df) == "NDVI"] <- "trimble" # ndvi is too ambiguous
setDT(all_df) # make data.table

# Drop the missing values.
all_df <- all_df[!is.na(sensor_1),]

## Data Analysis ----
# Analysis 1.1: Rearrange for plotting:
ndvi_long <- all_df[,c("ID","day", "sensor_1", "sensor_2","trimble")] |>
  melt(id.vars = c("ID","day"),
       variable.name = "sensor",
       value.name = "ndvi")

ndvi_v2 <- all_df[,c("ID", "sensor_1", "sensor_2", "trimble","day")] |>
  melt(id.vars = c("ID","trimble","day"),
       variable.name = "sensor",
       value.name = "drone_ndvi")



##  Analysis 1.2: Test models ---"---"---"---"
model_sensor_1 <- lm(formula = sensor_1 ~ trimble, data = all_df) 
model_sensor_2 <- lm(formula =  sensor_2 ~ trimble, data = all_df)

modelSum_trimble <- rbind(
  tidy(model_sensor_1),
  tidy(model_sensor_2)
)


setDT(modelSum_trimble)

modelSum_trimble$sensor <- c(rep("Mapir Survey 3W",2),
                             rep("MicaSense RedEdge-MX",2)
)

modelSum_trimble$R2 <-  c(
  rep(summary(model_sensor_1)$r.squared,2), # Mapir camera model' R^2
  rep(summary(model_sensor_2)$r.squared,2)  # MicaSense camera R^2
)

modelSum_trimble$term <- rep(c("intercept","slope"),2)

```

```{r A2_5000points, include=FALSE}
# paths
r_paths <- list.files(path = "data/ndvimaps/",
                      full.names = TRUE,
                      pattern = ".tif")

r_names <- list.files(path = "data/ndvimaps/",
                      pattern = ".tif") |> (\(x) substr(x,start = 1, stop = nchar(x)-4))() 
point_paths <- "data/vectordata/random_points/points_5000_clean_data_area.shp"

# read data
vi_raster <- rast(r_paths)  
names(vi_raster) <- r_names
points_vec <- vect(point_paths)
buf_vec <- buffer(points_vec,0.1) # 10 cm radius buffer (0.1 meter)
#extract ndvi at the random points in the field:
vi_points <- extract(x = vi_raster,
                     y = buf_vec,
                     fun = mean,
                     na.rm = TRUE)
setDT(vi_points)

# Correlation: 
## mica ~ mapir 2022-07-05
cor_0705 <- cor(x = vi_points$ndvimaps_mica0705,
                y = vi_points$ndvimaps_mapir0705)

## mica ~ mapir 2022-08-17
cor_0817 <- cor(x = vi_points$ndvimaps_mica0817,
                y = vi_points$ndvimaps_mapir0817)

##  Analysis 1.2: Test models ---"---"---"---"
model_day_1 <- lm(formula = ndvimaps_mica0705 ~ ndvimaps_mapir0705, data = vi_points) 
model_day_2 <- lm(formula = ndvimaps_mica0817 ~ ndvimaps_mapir0817, data = vi_points)

modelSum_5k <- rbind(tidy(model_day_1),tidy(model_day_2))
setDT(modelSum_5k)
modelSum_5k$term <- c("intercept","slope","intercept","slope")
modelSum_5k$day <- c("Survey 1", "Survey 1", "Survey 2","Survey 2") 

```

```{r A3_histograms,include=FALSE}
# First for scene 1:
# Individual Images --
image_roi <- vect("data/vectordata/indiv_scene_1.gpkg")
ground_soil <- vect("data/vectordata/ground_soil.gpkg")
init_mica <- rast("data/individualimages/together-calibrated/1_mica_modified.tif") |> mask(image_roi[1,])
init_mapir <- rast("data/individualimages/together-calibrated/1_mapir_modified.tif") |> project(init_mica) |> mask(image_roi[1,]) 


names(init_mica) <- c("mic_Blue", "mic_Green", "mic_Red", "mic_NIR", "mic_RedEdge")
names(init_mapir) <- c("map_Orange", "map_Cyan", "map_NIR")
# These images are 16 bit integer reflectance images. scale to reflectance
# by dividing with 2^16

refl_mica <- init_mica/(2^16)
refl_mapir <- init_mapir/(2^16)


## divide ground and soil. 
mic_ground <- mask(refl_mica, ground_soil[ground_soil$Crop == FALSE,])
mic_crop <- mask(refl_mica, ground_soil[ground_soil$Crop == TRUE,])

map_ground <- mask(refl_mapir, ground_soil[ground_soil$Crop == FALSE,])
map_crop <- mask(refl_mapir, ground_soil[ground_soil$Crop == TRUE,])

# save it all into a data
ground_df <- c(map_ground[[c("map_Red","map_NIR")]],
               mic_ground[[c("mic_Red","mic_NIR")]]) |> 
  as.data.table() |> melt(measure.vars = 1:4)  

ground_df$type = "ground"

## Do the same thing for plants, and add together in data.frame
crop_df <- c(map_crop[[c("map_Red","map_NIR")]],
             mic_crop[[c("mic_Red","mic_NIR")]]) |> 
  as.data.table() |> melt(measure.vars = 1:4)  

crop_df$type = "crop"

camera_reflectances <- rbind(ground_df,crop_df)
  
## Add column for sensor, use partial string match (grepl) to return mapir or mica. with an if else statement.
camera_reflectances[, camera := fifelse(grepl("map",variable), "mapir","mica" )]
## do same for color (band)
camera_reflectances[, band := fifelse(grepl("Red",variable), "Red","NIR")]

# Get mean and sd of each band in the plant and soil areas.
band_summaries <- camera_reflectances[, list(refl_av = mean(value, na.rm = T), refl_sd = sd(value,na.rm = T))
                                      , by =  list(type,camera,band)]

```

# Results

## NDVI at field sample points.

We measured NDVI with the three described sensors on two survey days in potato and soybean fields, adding up to 38 sample points. @fig-result_1 shows the NDVI values measured by each sensor for the two survey days. The figure shows that the infrared converted sensor reported lower NDVI, compared to the multispectral camera and Trimble Greenseeker handheld NDVI sensor. On both days the multispectral drone camera reported higher NDVI values than the handheld sensor The specific relationship between the sensor readings was evaluated with a least-squares linear model. The multispectral camera's NDVI readings scaled almost 1-1 with the handheld sensor (model slope = 0.77, @tbl-R1), but the camera's readings were higher (model intercept = 0.28, @tbl-R1). The linear model fit between the handheld and MicaSense sensor was better than the fit between the handheld and Mapir infrared converted sensor ($R^2= 0.88$ and $R^2=0.4$ respectively, @tbl-R1).

```{r result_fig_1}
#| label: fig-result_1
#| fig-cap: NDVI values measured by the three sensors (indicated by point colour) at the sample locations for the first (a) and second (b) field survey days
#| fig.height: 4 
#| fig.width: 6.5
#| fig-dpi: 300

# survey 1
p1 <- ggplot(ndvi_long[day=="survey 1",], aes(x = factor(ID),y = ndvi, colour = factor(sensor))) + 
  geom_point( alpha = 1, size = 2) +
  scale_colour_discrete(labels = c("Modified RGB", "Multispectral", "Handheld NDVI"))+
  xlab("Sample ID") + ylab("NDVI") +
  labs(colour = "Sensor")
# survey 2
p2 <- ggplot(ndvi_long[day=="survey 2",], aes(x = factor(ID),y = ndvi, colour = factor(sensor))) + 
  geom_point( alpha = 1, size = 2) +
  scale_colour_discrete(labels = c("Modified RGB", "Multispectral", "Handheld NDVI"))+
  xlab("Sample ID") + ylab("NDVI") +
  labs(colour = "Sensor")

p1/p2 + plot_annotation(tag_levels = "a") + plot_layout(guides = "collect")
```

```{r result_tbl_R1}
#| tbl-cap: Linear Model coefficients for model between drone sensor NDVI and handheld NDVI reading
#| label: tbl-R1
# KableExtra table
tmp <- 3
modelSum_trimble[,c("sensor","term","estimate","p.value","R2")] %>% 
  kbl(digits = 4) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  footnote(general = "n = 38",general_title = "")

```

In @fig-result_2 the black diagonal line represents the 1-1 line between drone-based measurements and the handheld sensor's NDVI measurements for all sample points. The low NDVI measured by the infrared converted camera can be seen by the red line lying below the diagonal line, with a shallow slope (slope = 0.14,@tbl-R1) . The multispectral sensor (line in blue) measured NDVI closer to the 1-1 line, but consistently measured higher NDVI than the handheld sensor.

```{r result_fig_2, warning=TRUE,message=FALSE}
#| label: fig-result_2
#| fig-dpi: 300
#| fig-cap: Relationship between NDVI measured by hand-held sensor (x-axis) and the two drone sensors (y-axis) at the 38 sample points measured over the 2 field surveys
ggplot(ndvi_v2, aes(x = trimble, y = drone_ndvi, colour = sensor)) + 
  geom_point(size = 2) +
  geom_abline(slope = 1,intercept = 0) + 
  geom_smooth(method = "lm")+
  xlim(-0.3,1) + 
  ylim(-0.3,1) + 
  scale_colour_discrete(labels = c("Inf. Conv.", "Multispectral")) +
  xlab("Handheld Sensor NDVI") + 
  ylab ("Drone Sensor NDVI") + 
  labs(colour = "Drone Sensor")
```

## Comparison of NDVI across field

Considering NDVI extracted from 5000 random point locations in the field, the relationship between NDVI measured by the multispectral and infrared converted camera is similar to the case were the field sampled locations were used (@fig-result_1). For both study dates the NDVI values by infrared converted camera were lower than the values measured by the multispectral sensor (@fig-result_3), and the range of values is also lower, suggesting that the infrared converted camera is less sensitive to variation in the plant condition, as quantified by NDVI index. Despite the difference in the scale of the values, there is still however high correlation between the two camera sensors' NDVI readings (survey 1: $r = 0.95$; survey 2: $r = 0.87$).

```{r warning=FALSE, message=FALSE}
#| label: fig-result_3
#| fig-cap: NDVI values measured at 5000 random points in the study-area, compared between the multispectral (x-axis) and infrared converted (y-axis) cameras. Fitted line is a least-squares linear model. (a) is for the first survey (2022/07/05) and (b) for the second survey (2022/08/17) 
#| fig.height: 4  
#| fig.width: 7
#| fig-dpi: 300

p1 <- ggplot(vi_points, aes(x = ndvimaps_mica0705, y = ndvimaps_mapir0705)) +
  geom_point(size = 2,alpha = 0.6) + 
  geom_smooth(method = "lm") +
  geom_abline(slope = 1,intercept = 0) + 
  xlim(-0.25,1.12) + 
  ylim(-0.25,1.12) + 
  labs(x = "NDVI: MicaSense RedEdge-MX",
       y = "NDVI: Mapir Survey 3W OCN")

p2 <- ggplot(vi_points, aes(x = ndvimaps_mica0817, y = ndvimaps_mapir0817)) +
  geom_point(size = 2,alpha = 0.6) + geom_smooth(method = "lm") +
  geom_abline(slope = 1,intercept = 0) + 
  xlim(-0.25,1.12) + 
  ylim(-0.25,1.12) +
  labs(x = "NDVI: MicaSense RedEdge-MX",
       y = "NDVI: Mapir Survey 3W OCN")
p1+p2 + plot_annotation(tag_levels = "a") + plot_layout(guides = "collect")

```

## Individual band reflectances

To investigate the cause of the difference in measured NDVI, we consider the reflectance in red and near-infrared bands for a scene in the potato field. For the NIR-band the MicaSense RedEdge-MX measures much higher reflectance over vegetation than the Mapir Survey3W camera ( @fig-result_4). However, the RedEdge-MX camera measured lower reflectance in the red-channel than the Survey3W. If the MicaSense sensor's readings are considered accurate, it thus suggests that the infrared converted sensor captures more noise, since it overestimates red in a scene were red values are actually low, and underestimates NIR. Based on the formula for NDVI ( @eq-ndvi), overestimating red and underestimating near-infrared will lead to underestimates for NDVI, as observed ( @fig-result_1). For ground pixels the NIR and red reflectance measurements are however similar between the sensors.

```{r }
#| eval: false
#| include: false
#| label: fig-result_reflmap
#| fig-dpi: 300

# #| fig-cap: Band reflectance as captured by drone camera over the same area in a potato field, for red band of the infrared converted (a) and multi-sensor (b) cameras , and the near-infrared band for the infrared converted (c) and multi-sensor (d) cameras.  The color scale is shared for the images of the same band. 
 #| fig-subcap:
 #|  - "Red: Mapir"
 #|  - "Red: MicaSense"
 #|  - "NIR: Mapir"
 #|  - "NIR: MicaSense"
 #| layout-ncol: 2
 #| layout-nrow: 2


plot(refl_mapir$Red,range = c(0,0.4),legend = FALSE, mar = c(2.1, 2.1, 2.1, 2.1))
plot(refl_mica$Red,range = c(0,0.4), mar = c(2.1, 2.1, 2.1, 4.5))

plot(refl_mapir$NIR,range = c(0,1),legend = FALSE, mar = c(2.1, 2.1, 2.1, 2.1))
plot(refl_mica$NIR,range = c(0,1), mar = c(2.1, 2.1, 2.1, 4.5))

```

```{r 2_image_histogram}
#| label: fig-result_4
#| fig-cap:  Density plots of reflectance in the near-infrared and red channels, as captured by the infrared converted and multispectral sensors. Result shown separately for crop covered and ground areas in the field
#| fig.width: 7
#| fig.height: 5
#| fig-dpi: 300
ggplot(camera_reflectances, aes(x=value, fill = camera)) + 
  geom_density(alpha = 0.7) +
  scale_fill_discrete(labels = c("Infrared converted", "Multispectral"))+
  facet_grid(rows = vars(band), cols = vars(type),scales = "free") + 
  geom_vline(data = band_summaries, aes(xintercept = refl_av)) +
  labs(x = "Reflectance",
       y = "Density",
       fill = "Sensor")  

```

```{r scene_1_redbands}
#| eval: false
# This is the code to save individual images. But, I added these together to make a new figure with scribus. 
# rowwe idee:
    # is the mapir orange influenced by green? So that mapir is measuring some green in orange band?
    # if we assume that 
    # take mapir$orange - mica$red 
    # is the difference in band values showing large values in plant area?
mapir_green <- refl_mapir$map_Red - refl_mica$mic_Red

bandfig_titles <- c("(a) IC Red - MS Red", "(b) MS: Red band", "(c) IC: Red Band" )

png(filename = "processing/scene_1_RGB.png")
plotRGB(refl_mica, r = 3, g = 2, b = 1, scale = 0.4)
dev.off()

# red difference:
png(filename = "processing/scene_1_reddifdf.png")
plot(mapir_green, axes = FALSE, range = c(0,0.3), legend = FALSE)
dev.off()

# Mapir red:
png(filename = "processing/scene_1_Map_red.png")
plot(refl_mapir$map_Red, axes = FALSE, range = c(0,0.3), legend = FALSE)
dev.off()

# Mic red:
png(filename = "processing/scene_1_Mic_red.png")
plot(refl_mica$mic_Red, axes = FALSE, range = c(0,0.3), legend = FALSE)
dev.off()

# For getting legend.
e <- ext(refl_mapir) + 0.00001
png(filename = "processing/scene_1_legend.png", res = 96)
plot(refl_mica$mic_Red, axes = FALSE, 
     range = c(0,0.3),
     plg = list(loc = "bottom"))
dev.off()

```

The notion that the Mapir camera's red channel (615 *nm*: 'orange') is capturing reflectance from the neighbouring wavelengths like green is strengthened by the result that there is a strong difference in red reflectance captured by the two sensors over vegetated areas in field (@fig-scene1red b). The Mapir sensor captured high reflectance in red channel over potato plants (@fig-scene1red c), if compared to the red reflectance captured by the MicaSense sensor (@fig-scene1red d). However, for pixels of exposed ground, the reflectance is similar between the sensors. This may be because there is no green light reflecting from the ground that can pollute the measurement of the Mapir camera's red channel.

![The difference in red reflectance captured by the infrared converted and multispectral camera. Shown for a scene over the potato field, with (a) RGB image, (b) the difference between Mapir and MicaSense red channels, (c) Mapir red reflectance, and (d) MicaSense red reflectance](reporting/images/scene_1-page001.png){#fig-scene1red}

# Discussion

In this case study it was observed that the Mapir Survey3W OCN camera --- a commercial infrared converted camera marketed for use in agriculture --- consistently measured lower NDVI values in a crop field, compared to a more expensive multi-sensor multispectral camera, and a handheld NDVI meter. This result reflects those of the study by @gomes2021, who also observed lower NDVI measurements from Mapir Survey 3W camera, compared to multi-sensor and Greenseeker handheld sensor, and also [@argolodossantos2020] who noted Mapir Survey RGNIR measured lower NDVI in a maize field than reported by other studies. A possible explanation for this observation may be that the camera considered in our study overestimated reflectance in the red-channel (@fig-result_4). The Survey 3W OCN uses a orange band in the place of red, thus having a central wavelength nearer to green wavelengths than common for red bands. This fact, together with the broad band-sensitivities of single-sensor RGB cameras [@burggraaff2019; @berra2015] might cause the orange band to capture too much light from the green part of the spectrum ( @fig-scene1red ). Since vegetation also reflects green light, the orange channel might be measuring too high reflectance over vegetation leading to underestimation of NDVI measurements (@fig-result_1). If this is the case, it will be better to use infrared converted cameras with red band centred further away from the green wavelengths. Yet, the camera manufacturer supplied wavelength sensitivity chart shows no overlap in the sensitivities of the Survey 3W OCN camera's orange band with green wavelengths [@mapira]. A possible alternative explanation is that the incorrect measurements are caused by a problem in the calibration of images to reflectance. This could happen if the lighting on the calibration target during its capture did not represent the lighting over the field during the flight because of shadow or cloud. However, the fact that similar results were observed on separate field survey days, and also by @gomes2021, who used a different calibration approach, suggests that calibration errors are not the sole cause of the error in the NDVI reading. And also, the fact that over the same scene of crops, the Mapir camera had lower near-infrared, but higher red reflectance than the multispectral MicaSense camera ( @fig-result_4 ), further suggests that the errors are caused by the sensor, rather than the calibration. The study by @nijland2014 investigated the utility of infrared converted cameras for vegetation monitoring, but they also found inadequate band separation to reduce the accuracy of reflectance measured by these cameras. If inadequate band separation is to blame for inaccurate readings, a camera modified to exclusively measure a vegetation index like NDVI might improve results by using an additional filter that blocks green light, and so reduces contamination into the red and near-infrared bands. Also, filters that are sensitive to red light further away from the green bands (longer wavelengths) may help to reduce noise, compared to the Orange (615 nm) filter used in our study.

@vonbueren2015 suggest that vegetation indices from RGB and infrared converted cameras are most suitable for simple (qualitative) assessment of vegetation condition over a large area. In our study we noted high correlation between VI measurements made between the multispectral and infrared converted sensor (r = 0.96 and r = 0.87 for the two survey days respectively), similar to @gomes2021 and @vonbueren2015. This indicates that most of the variation in crop condition that is measured by the multispectral sensor is also captured in the lower-cost infrared converted camera. So, the infrared converted camera may successfully be used to qualitatively assess the variation in vegetation status across the study area. Such qualitative monitoring might help farm managers to easily identify specific problem areas that can then be further investigated in-field. However, the results of this study suggest that the infrared converted camera considered might not be suitable to inform quantitative decisions like fertiliser application rate in Variable Rate Application systems [@alley2011] or biomass/yield estimation models that are calibrated for accurate NDVI measurements.

If infrared converted cameras are indeed mostly suitable for qualitative type measurement, it brings to question whether such converted cameras hold an advantage over unmodified RGB cameras that are generally even cheaper than infrared converted cameras. Regular RGB cameras attached to drones have indeed successfully been used for biomass and yield estimation studies [@li2018; @argolodossantos2020; @bendig2014], and for estimating biomass of green algae in the ocean [@xu2018]. Some of these studies utilized the fact that RGB cameras capture comparatively high resolution images, which are used to generate three dimensional models of vegetation canopy. This 3-D information, together with visible-band vegetation indices may be sufficient for calculating crop biomass, which in turn can be related to expected yield in some crops, or area covered by euthrophication in water bodies. These visible band indices might not be as sensitive to variation in growth vigour as indices capturing near-infrared (e.g., NDVI), but they are suitable for distinguishing crops from background [@riehle2020], and thus for estimating the area of crop cover or biomass related parameters. Also, because of the shortcomings of single-sensor cameras to accurately capture near-infrared wavelengths, vegetation monitoring using just regular RGB cameras, coupled with visible band indices, might be better than infrared converted cameras . @nijland2014 observed that an unmodified RGB camera could quantify crop condition more accurately than the infrared converted camera, despite the theoretical advantage of the converted camera to capture infrared wavelengths. @fernandez-figueroa2022 also tested an infrared converted Mapir Survey 3W for measuring Chl *a* concentration in ocean algal blooms, and found that the converted camera's vegetation indices correlated worse to measured Chl *a* than visible band indices captured by a regular RGB camera.

**Conclusion**

In conclusion then, our study observed that NDVI measured by a infrared converted did not agree well with a more expensive (multi-sensor) multispectral camera, and so should be used cautiously for quantitative applications like determining fertiliser application rates in crop fields. The inaccuracy might be caused by too broad band sensitivities, leading to the red channel capturing light from the green wavelengths. This problem might further be exacerbated by the orange (615 nm) filter used for the red channel in the Survey 3W OCN camera, which is itself nearer to green wavelengths then most other infrared converted or multispectral drone cameras. However, since the infrared converted camera's NDVI values still correlated well with measurements or multispectral sensor, it can be useful for qualitative assessment of comparative vegetation condition to easily identify problem areas. However, for such applications unmodified RGB cameras with visible band indices might be equally suitable. Finally, low-cost RGB or modified cameras might still be suitable for some quantitative applications like biomass estimation, if care is taken to use methods tailored to the advantages and limitations of these sensors.

# References
